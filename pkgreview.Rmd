#  (PART) Package Review {-}


# Package Review {#pkgreview}

The current chapter should be considered an extension of the corresponding
["Guide for Reviewers"](https://devguide.ropensci.org/reviewerguide.html) in
rOpenSci's "Dev Guide". The principles for reviewing packages described there
also apply to statistical packages, with this chapter describing additional
processes and practices for review of packages submitted to the statistical
software peer review system. Being a direct extension, the [template for
general software review](https://devguide.ropensci.org/reviewtemplate.html) is
to be used allowing with the following additional considerations and components
to be entered into a review.

## Package badging


This system for peer-review of statistical software features badges in three
categories of **standard**, **silver**, and **gold**. As described in the
corresponding chapter for [package developers](#pkgdev-badges), these are:

1. A **standard** or default badge for software which has been accepted, and
   which reviewers have deemed sufficiently compliant with relevant standards.
3. A **silver** badge for software for which developers aspire to reach gold
   standard, but which is not yet sufficiently compliant. See below for
   details.
2. A **gold** badge for software which complies with *all* standards which
   reviewers have deemed potentially applicable. This is the highest standard,
   and requires developers to ensure compliance with the largest number of
   standards, while minimising the number of standards deemed non-applicable.

Developers will state which badge they hope to attain in a [`lifecycle.md`
file](#pkgdev-badges) within their repository. All reviews should complete the
follow procedures for review of packages aiming for a **standard** badge.
Packages aiming for **silver** or **gold** badges should then be reviewed
according to the additional criteria described in the subsequent sub-sections.

## General Review Requirements

The primary way that reviews of statistical software differ from reviews of
software submitted to rOpenSci's general peer-review system is that the statistical
software system includes lists of general and category-specific standards. The
first important task of reviews is thus to assess the compliance of software
with these standards. The standards themselves are contained in the [subsequent
chapter](#standards), which both developers and reviewers will need to refer to
throughout package development and review. For software to be considered within
scope, it must be described by one or more of the categories of statistical
software for which standards have been developed. Software will then need to
comply both with the *General Standards* for statistical software, and with
specific sets of standards for each applicable category. These categories will
have been discussed and allocated in the pre-review phase, and categorical
applicability should generally not have to be considered during an actual
review.


### Assessment against standards

The process of assessing software against standards is facilitated by the
[`srr` (**s**oftware **r**eview **r**oclets)
package](https://github.com/ropenscilabs/srr) which both developers and
reviewers will need to install with the following line:

```{r srr-install-rev, eval = FALSE, echo = TRUE}
remotes::install_github("ropenscilabs/srr")
```

This package is primarily intended to aid developers in documenting both how
and where their software complies with each of the relevant general and
category-specific standards. Reviewers can then clone a local copy of the
repository to be reviewed, and in the root directory of that repository, run
the [`srr_report()`
function](https://ropenscilabs.github.io/srr/reference/srr_report.html)
to generate a hyperlinked `html` report of standards compliance.

Reviewers are requested to click on every single link which appears in that
report, and to at least briefly assess whether they believe the software at
each location complies with the nominated standards. The report itself is
divided into two main sections, named after the [roclet
tags](https://ropenscilabs.github.io/srr/articles/srr-stats.html#3-roxygen2-tags)
of:

1. `@srrstats` for standards with which software complies;
2. `@srrstatsNA` for standards which developers have deemed not to be
   applicable to their software.


Each of those two sections will then be divided into sub-sections according to
where within the repository those standards are reported (generally meaning
which sub-directory, such as `R/`, `tests`/, or elsewhere). No action need be
taken on standards with which reviewers agree, whether because software
complies and has a tag of `@srrstats`, or because a standard is not applicable
and has a tag of `@srrstatsNA`. Reviewers are only asked to note any standards
with which they disagree, primarily either because of:

1. Disagreement in standards compliance, where developers have used a tag of
   `@srrstats` but a reviewer judges either the explanation or associated code
   to be insufficient for compliance; or
2. Disagreement about non-applicability of a standard, where developers have
   used a tag of `@srrstatsNA`, but a reviewer believes that standard ought to
   apply to the software.

Please progress through the entire report generated by the [`srr_report()`
function](https://ropenscilabs.github.io/srr/reference/srr_report.html), and
note all instances of disagreement, generally grouped into the two categories
described above. Reports of potential disagreements in standards compliance
can be included directly within a review issue, or reviewers can opt to
initially work directly with developers to resolve issues directly within the
code itself, as described in the following sub-section.

Note that this initial assessment against standards is intended only to clarify
and resolve statements of compliance with which reviewers disagree, and in
particular should be conducted entirely independent of whether or not software
may be aspiring to silver or gold badges. Procedures for those latter cases are
conducted in subsequent review phases, as described below.


#### Optional alternative workflow via pull request

The [`srr` package](https://ropenscilabs.github.io/srr) also facilitates a more
direct way for reviewers to respond to potential areas of disagreement in
standards compliance, through reviewers modifying the code and submitting a
pull request. This procedure is entirely optional, although doing so helps to
keep review issues largely free of technical discussions about standards
compliance. Reviewers willing to submit direct pull requests can do so via the
following steps:

1. Modify your local copy of the repository by changing any disputed tags,
   whether `@srrstats` or `@srrstatsNA`, to `@srrstatsTODO`, which is the tag
   used to denote standards which have not yet been complied with.
2. Add an additional note within that section of the documentation explaining
   why you disagree with the developers claim of compliance (via `@srrstats`)
   or non-applicability (via `@srrstatsNA`).
3. Ensure that your commit message includes a reference to the review issue
   (like `"standards review for
   ropenscilabs/statistical-software-review#<issue_num>"`).
4. Submit a pull request directly to the developer's repository.
5. Use the pull request to discuss resolution of disputed standards with
   developers.

These discussions may lead to modifications of a pull request, if a reviewer
is convinced by developers that standards have indeed been complied with, or
are indeed not applicable. Either way, at the end of such discussions, this
pull request should ultimately include several versions of tags reverted to 
`@srrstatsTODO`. After merging, developers will then have to once again address
all of these tags, and to either comply or deem them not applicable, before
review can proceed any further.

Reviewers are encouraged to opt for this workflow, because it enables them to
work together with developers to initially ensure complete agreement on
standards compliance prior to actually proceeding with the formal review.

### Assessment of algorithmic quality

The overall quality of statistical software must ultimately reflect the quality
of core statistical algorithms, whether in terms of appropriateness of
implementation, efficiency, uniqueness, or other qualitative aspects. This
project attempts to set standardised expectations of many aspects of
statistical software, yet core statistical algorithms generally remain too
diverse to be considered or compared in any standardised way. We thus request
all reviewers to explicitly review the quality of the core statistical
algorithms contained within a package. Most category-specific standards include
a central "*Algorithmic Standards*" component which can be used to frame broad
considerations for algorithmic quality. The [*General Standard*
**G1.1**](#general-standards) also requires all similar algorithms or
implementations to be documented within the software, so reviewers should also
have access to a list of comparable implementations.

Extending from those two pieces of information, reviewers are requested to
consider in as much depth as possible the overall quality of the core
statistical algorithm(s), paying particular attention to the direct
implementations in code.

### General Package Review

Beyond the preceding considerations which are specific to statistical software,
a review should generally reflect the processes established in rOpenSci's
general software review system, for which the best source of information is
provided by [reviews
themselves](https://github.com/ropensci/software-review/issues), along with the
[*Guide for Reviewers*](https://devguide.ropensci.org/reviewerguide.html).

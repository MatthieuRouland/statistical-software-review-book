#  (PART) Package Review {-}


# Package Review {#pkgreview}

The review processes of rOpenSci and JOSS are qualitatively different, with
JOSS submissions guided by extensive automation, and so being strongly
determined by their
[checklist](https://github.com/openjournals/joss/blob/master/docs/review_checklist.md),
whereas rOpenSci reviews are commenced only after authors complete the
[checklist](https://github.com/ropensci/software-review/blob/master/.github/ISSUE_TEMPLATE/A-submit-software-for-review.md)
(or otherwise explain any anomalies). Reviewers of submissions to rOpenSci are
solicited privately, and privately informed both to read the [*Guide for
Reviewers* chapter](https://devguide.ropensci.org/reviewerguide.html) in the
[*Development, Maintenance, and Peer Review*
Guide](https://devguide.ropensci.org), and that their review must be submitted
with the [*Review
Template*](https://devguide.ropensci.org/reviewtemplate.html#reviewtemplate).
This template serves the same purpose as the automatically-generated JOSS
template, but is to be pasted by authors themselves into their own comments in
the review issue, whereas the JOSS checklist is to be filled out by reviewers
editing the opening comment of the review issue. In short, the rOpenSci
checklist is an official starting point, with reviews submitted at the end with
the help of a template; the JOSS checklist is an official endpoint, empty at
first and progressively completed by each reviewer as they progress through the
review process. 

We envision a system primarily derived from rOpenSci's current system, with
reviews completed through the use of a template and pasted as comments at the
bottom of a github issue. This approach will face one immediate difficulty in
that templates will likely differ through software being described by different
combinations of the [categories described above](#scope). It may suffice to
combine a generic "master" template with category-specific items to be appended
according to the description of submitted software within our list of
categories, although it is important to note that this may exclude review
criteria reflecting unique combinations of categories (for example, a checklist
item appropriate for the visualisation of results from ML algorithms).

The preceding consideration exemplifies the extent to which processes developed
and employed to review statistical software are likely to be strongly
influenced by the kinds of automated tools we develop, both for automated and
self assessment along with associated reporting systems, as well as potentially
for more comprehensive assessments and reporting systems or standards not
otherwise amenable to automation. In the current initial phase of this project
prior to the concrete development of any of these kind of tools, the present
considerations of the review process are accordingly and necessarily generic in
nature. We anticipate this current sub-section becoming particularly more
detailed as the project progress and as we develop project-specific tools for
software assessment.

### Review Templates

As described above, the [JOSS
checklist](https://github.com/openjournals/joss/blob/master/docs/review_checklist.md)
is pre-generated with the opening of each review issue, whereas the [rOpenSci
template](https://devguide.ropensci.org/reviewtemplate.html#reviewtemplate) is
to be completed and pasted in to the issue by reviewers. The two templates are
nevertheless broadly similar, both including the following checklist items:

- [ ] The reviewer has no conflict of interests

**Documentation** The software has:

- [ ] A clear statement of need
- [ ] Installation instructions
- [ ] Function documentation
- [ ] Examples
- [ ] Community guidelines for how to contribute

**Functionality** The software should:

- [ ] Install as documented
- [ ] Meet its own functional claims
- [ ] Meet its own performance claims
- [ ] Have automated tests (considered by JOSS as part of "Documentation")

In addition, JOSS requires reviewers:

- [ ] To agree to abide by a reviewer [code of
   conduct](https://joss.theoj.org/about#code_of_conduct)
- [ ] To confirm that the source code is in the nominated repository
- [ ] To confirm that the software has an appropriate license.
- [ ] To confirm that the submitting author has made major contributions, and that
   the provided list of authors seems appropriate and complete.

rOpenSci insists in turn on the two additional aspects, that software  

- [ ] Has a vignette demonstrating major functionality; and
- [ ] Conforms to the rOpenSci packaging guidelines

Perhaps the most influential difference between the two systems is that the
rOpenSci template concludes with the following lines:

---

### Review Comments

The section break and sub-section heading act in combination as a prompt for
reviewers to add their own discursive comments, whereas the JOSS template has
no such field. Accordingly, the majority[^joss-review-claim] of JOSS reviews
merely consist of a completed checklist, whereas *all* rOpenSci reviews are
extensively discursive, with reviewers frequently offering very extensive
comments and analyses of submitted code.

[^joss-review-claim]: That claim has not been substantiated.

These differences may plausibly be interpreted to reflect general differences
in the *cultural practices* of the two review systems, with rOpenSci having
particularly nurtured the cultural practice of extensively discursive reviews,
notably through suggesting that prior reviews ought be perused for good
guidelines on review practices. We intend to continue to foster and encourage
such cultural practices, while at the same time aiming to develop a system for
more structured yet discursive input, in order both to provide more focussed
software reviews, and to lessen the burden on reviewers. We anticipate
commencing the development of such structure in subsequent iterations of the
present document.


### Category-Specific Aspects of Reviews

We defer consideration of category-specific aspects of review until we have
concluded a first round of consultation on the [preceding categorical
definitions](#scope).

### Reviewer Recommendations

Both rOpenSci and JOSS currently work with a binary recommendation scheme of
rejection or acceptance. In both cases, rejection is primarily decided in
response to a pre-preview (JOSS) or pre-submission enquiry (rOpenSci), and
usually for the reason of being out-of-scope (in rOpenSci's case because
software does not fit within the defined categories; and in JOSS's case because
the software does not have a specific research focus). Having obtained approval
to proceed from pre-review to full review, both systems generally work with
package authors to strive for ultimate acceptance. Rejections during this phase
generally only happen when authors stall or abandon ongoing or requested
package development. As long as authors continue to be engaged, reviews very
generally proceed until a submission is accepted.

***Proposal*** While a variety of potential outcomes of the review process are
considered [immediately below](#review-acceptance), reviewers will only be
requested to ultimately check a single box indicating their approval for
software to be accepted. An approved submission may then receive a variety of
labels in response to binary acceptance, as described below.

---

***Proposal***

1. We adopt the current rOpenSci approach of having reviews based on
   a pre-defined template to be completed by reviewers and pasted as the latest
   comment in a review issue, rather than the JOSS model of having reviewers
   edit the initial, opening comment of a review issue.
2. We adopt the current rOpenSci approach of having reviewers testify the time
   spent on their review. We either then:
   (i) Do not provide any information on typical times devoted by other
       reviewers; or
   (ii) Provide summary information including estimates of variation and
        proviso that such information is only intended to avoid reviewers
        otherwise feeling obliged to devote unnecessarily *long* times to
        reviews.
3. We adopt and adapt the general review templates currently used by rOpenSci
   and JOSS, extending both in order to provides as much structured discursive
   feedback as possible.
4. We develop at least examples of category-specific template items to be added
   to the general review template.

## Acceptance / Scoring / Badging {#review-acceptance}

Software being recommended for acceptance by reviews need not be reflected in
a simple "accepted" label. Particularly in the early stages of our system for
peer-reviewing statistical software, we may have some kind of checklist from
which we require authors to ultimately comply with some recommended limited
number of items, yet not all. It may then be worthwhile have a review outcome
that flags this compliance level, and indicates that software will be expected
to retain compliance as our system develops.

Another example may be outcomes which consider the kinds of life cycle models
considered above, in which context it may be useful to have an outcome that
labels software as having passed initial or primary review, yet which will
still be subject to subsequent review some agreed-upon time later. Such systems
of re-assessment will nevertheless not necessarily be (equally) applicable to
all submissions, and so such "progressive labelling" will likely only ever be
optional, and applicable where appropriate.

***Proposal*** We implement a recommendation system which explicitly flags the
version of our system's standards with which reviewed software complies.

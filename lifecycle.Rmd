
# Lifecycles of Software and of Peer Review {#lifecycle}

Prior to considering standards for software assessment, it is important to
consider "lifecycles", both of software itself, and of the peer-review process.
The importance of considering Software "lifecycles" has long been recognized for
closed-source proprietary software, yet lifecycles have only been given scant
consideration in contexts of open source software [exceptions include
@stokes_21_2012;@lenhardt_data_2014]. A long history and tradition in both
practice and published literature on software review [for example,
@mili_software_2015;@ammann_introduction_2017] generally concludes that
software review is most effective when it is an ongoing process that is
structurally embedded within a software lifecycle, and when review occurs as
frequently as possible. Such a practice contrasts strongly with the singular
nature of review as currently implemented by rOpenSci.

An effective system for peer review of statistical software is thus may lie
somewhere between the "one-off" practices above, and frequent, ongoing review
typical of software development in active teams. An [analysis of the effects of
rOpenSci's review process on a few metrics of software development
activity](https://github.com/mpadge/statistical-software/tree/master/ros-review-effects)
revealed that software development tends to stagnate following review. This may
be interpreted to reflect software having reached a sufficiently stable state
requiring relatively little ongoing maintenance. However, we note that metrics
of community engagement with software are generally positively related to the
metrics of development activity considered there. Slowing of software
development following review may also accordingly reflect or result in decreases
in community engagement.

Potential systems to enhance review of the kind current practiced by rOpenSci,
and particularly to encourage and enable more ongoing review on smaller scales
and shorter time frames, include pull-request reviews, and systems for providing
inline code reviews (such as
[watson-ruby](https://github.com/nhmood/watson-ruby)). In addition, ongoing
"review" may be explicit in considering the role of user feedback, for instance,
in defining and updating the scope of statistical routines (see "Standards for
Statistical Software" below).

Aspects of lifecycle in both of these regards are particularly important
because some of the standards described in the following chapter may be only be
applicable before, during, or after some specific lifecycle phase, whether of
software development, the review process, or both.


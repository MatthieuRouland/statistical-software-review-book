```{r include=FALSE, cache=FALSE}
knitr::opts_chunk$set(
  cache = TRUE,
  echo = FALSE
)
```
#  (PART) Scope and Standards {-}

```{r startup, echo = FALSE, message = FALSE}
library (dplyr)
library (rvest)
library (igraph)
library (ggplot2)
theme_set (theme_minimal ())
```

# <span style="color:red;">Scope and Standards (*Seeking Feedback*)<span> {#scope}

One task in extending the rOpenSci peer review system to statistical software
is defining _scope_ - what software is included or excluded. Defining scope
requires some grouping of packages into categories.  These categories play key
roles in the peer review process and standards-setting

1. Categorical definitions can determine which kinds of software will be admitted;
2. Different categories of software will be subject to different standards, so
   categories are key to developing standards, review guidance, and automated
   testing.

Creating a categorization or ontology of statistical software can easily become
an overwhelming project in itself. Here we attempt to derive categories or
descriptors which are *practically useful* in the standards and review process,
rather than having formal structure. We use a mix of empirical research and
subjective judgement (and strive to minimise the latter in favour of the former).

We consider two main types of categories:

1. Categories of software structure, referred to as "software types",
   determined by computer languages and package formats in those languages; and
2. Categories defining different types of statistical software, referred to as
   "statistical categories".

## Software types {#software-types}

### Languages

This project extends existing an software peer-review process run by rOpenSci
[rOpenSci](https://ropensci.org), and is primarily intended to target the **R**
language. Nonetheless, given the popularity of Python in the field (see
relevant analyses and notes in [Appendix A](#python)), the impact of developing
standards applicable to Python packages must be considered.  rOpenSci also has
a close collaboration with its sister organization,
[pyOpenSci](https://www.pyopensci.org).

In addition it is particularly important to note that many **R** packages
include code from a variety of other languages. The following table summarises
statistics for the top ten languages from all `r format (nrow
(available.packages ()), big.mark = ",")` [CRAN](https://cran.r-project.org)
packages as of `r format(Sys.time(), "%a %b %d %Y")` (including only code from
the `/R`, `/src`, and `/inst` directories of each package).

```{r language-table, message = FALSE, echo = FALSE}
library (dplyr)
x <- readRDS ("./scripts/cran-cloc.Rds") %>%
    select (language, loc) %>%
    mutate (proportion = loc / sum (loc)) %>%
    rename ("lines" = loc)
clines <- sum (x$lines [grep ("^C", x$language [1:8])])
weblines <- sum (x$lines [x$language %in% c ("HTML", "JavaScript", "CSS")])
xf <- mutate (x, lines = format (lines, big.mark = ","))
knitr::kable (xf [1:10, ], digits = c (NA, 0, 3), caption = "Proportion of code lines in different languages in all CRAN packages.")
```

Close to one half of all code in all R packages to date has been written in the
R language, clearly justifying a primary focus upon that language. Collating
all possible ways of packaging and combining C and C++ code yields
`r format (clines, big.mark = ",")` lines or code or
`r round (100 * clines / sum (x$lines))`% of all code, indicating that
`r round (100 * (clines + x$lines [1]) / sum (x$lines))`% of all code has been
written in either R or C/C++. Three of these top ten languages are likely
related to web-based output (HTML, JavaScript, and CSS), representing a total
of `r round (100 * weblines / sum (x$lines))`% of all code. While this is
clearly a significant proportion, and while this *may* reflect an equivalent
high frequency of code devoted to some form of web-based visualisation, these
statistics represent *all* R packages. There is no simple way to identify which
of these might be considered statistical packages, but knowing that there are
packages exclusively constructed to generate web-based visualisations in
a generic sense suggests that this value ought be taken as an upper limit on
the likely frequency of visualisation packages (or parts thereof) in the
context of statistical software.





***Key considerations***:

- Expansion into the Python ecosystem has great potential for impact, but goes
  beyond the general areas of expertise in the core ecosystem. (And Python code
  represents just
  `r format (x$lines [grep ("^Python", x$language)], big.mark = ",")`
  lines of code, or
  `r format (100 * x$proportion [grep ("^Python", x$language)], digits = 1)`%
  of all code within all R packages.)
- Compiled languages within R packages are core to many statistical
  applications; excluding them would exclude core functionality the project
  aims to addressed. The majority of compiled code is nevertheless C and/or
  C++, with Fortran representing under 2% of all code.
- Languages used for web-based visualisations comprise a significant proportion
  (`r round (100 * weblines / sum (x$lines))`%) of all code. While this
  potentially indicates a likely importance of visualisation routines, this
  figure reflects general code in all R packages, and the corresponding
  proportion within the specific context of statistical software may be
  considerably lower.

***Proposal***: Peer review in the system will primarily focus on code written
in R, C, and C++. Standards will be written so as to separate language-specific
and non-language-specific components with an eye towards further adoption by
other groups in the future (in particular groups focussed on the Python
language). Any decision to include visualisation software and routines within
our scope will likely entail an extension of linguistic scope to associated
languages (HTML, JavaScript, and maybe CSS).

### Structure

R has a [well-defined system](https://cran.r-project.org/doc/manuals/R-exts.html) for structuring software packages"
Other forms of packaging **R** software may nevertheless be considered within scope. These may include

1. Python-like systems of
[modules for **R**](https://github.com/klmr/modules).
2. Scripts, such as standalone comand-line scripts
3. Web applications such as Shiny packages

**Key considerations**: Allowing non-package forms of code into the peer review
system could potentially bring in a large pool of code typically published
alongside scientific manuscripts, and web applications are a growing, new area
of practice. However, there is far less standardization of code structure to
allow for style guidelines and automated testing in these cases.

**Proposal**: Peer review in the system will be limited to R packages, and
tools developed will be specific to R package structure, although keeping in
mind potential future adaptation and adaptability to non-packaged R code.
Standards that may apply to non-packaged are code may also be noted for use in
other contexts.

## Statistical Categories {#statistical-categories}

As alluded to at the outset of this chapter, a primary task of this project
will be to categorise statistical software in order to:

- Determine the extent to which software fits within scope
- Enable fields of application of software to be readily identified
- Enable determination of applicable standards
- Enable discernment of appropriate reviewers

Different categories of statistical software will likely have different
standards, yet there will nevertheless be general standards applicable
regardless of categories. General standards may simply require that a piece of
software is demonstrably superior to otherwise equivalent software in at least
one (or perhaps more?) specific way(s). Submissions may occasionally be truly
unique, and therefore devoid of any "otherwise equivalent software". It will
nevertheless be expected that otherwise equivalent software will generally
exist, in which cases such standards might accordingly be approached through
considering the following kinds of questions, made in regard to one or more
"reference implementations" of otherwise equivalent software, and which
software authors may be requested to specify:

- [ ] **Efficiency:** Is the software more efficient (faster, simpler, other
  interpretations of "efficient") than reference implementations?
- [ ] **Reproducibility or Reliability:** Does the software reproduce
  sufficiently similar results more frequently than reference implementations
  (or otherwise satisfy similar interpretations of reproducibility)?
- [ ] **Accuracy or Precision:** Is the software demonstrably more accurate or
  precise than reference implementations (such as ?
- [ ] **Simplicity of Use:** Is the software simpler to use than reference
  implementations?
- [ ] **Algorithmic Characteristics:** Does the algorithmic implementation
  offer characteristics (such as greater simplicity or sensitivity) superior to
  reference implementations? If so, which?
- [ ] **Convergence:** Does the software provide faster or otherwise better
  convergence properties than reference implementations?
- [ ] **Method Validity:** Does the software overcome demonstrable flaws in
  previous (reference) implementations? If so, how?
- [ ] **Method Appliciability:** Does the software enable a statistical method
  to be applied to a domain in which such application was not previously
  possible?
- [ ] **Automation** Does the software automate aspects of statistical analyses
  which previously (in a reference implementation) required manual intervention?
- [ ] **Input Data:** Does the software "open up" a method to input data
  previously unable to be treated by a particular algorithm or method?
- [ ] **Output Data:** Does the software provide output in forms previously
  unavailable by reference implementations?
- [ ] **Reference Standards:** Are there any reference standards, such as the
  US National Institute of Standards and Technology's
  [  collection of reference data sets](https://www.itl.nist.gov/div898/strd)
  against which the software may be compared? If so, which?

***Proposal*** Submissions will be required to check at least one of the
preceding items, to nominate at least one "reference implementation", and to
explain how the submitted software is superior (along with a possibility to
explain why software may be sufficiently unique that there is no reference
implementation, and so no claims of superiority can be made).

---

We now consider potential categories within the general domain of statistical
software. In order to derive a realistic categorisation, we used empirical data
from several sources of potential software submissions, including all
apparently "statistical" R packages published in the [Journal of Open Source
Software (JOSS](https://joss.theoj.org)), packages published in the [Journal of
Statistical Software](https://www.jstatsoft.org/index), software presented at
the 2018 and 2019 Joint Statistical Meetings (JSM), and Symposia on Data
Science and Statistics (SDSS), well as CRAN task views.

We allocated one or more key words (or phrases) to each abstract, and use the
frequencies and inter-connections between these to inform the following
categorisation are represented in the [interactive
graphic](https://ropenscilabs.github.io/statistical-software/abstracts/network-terms/index.html)
(also included in the [Appendix](#appendix-keywords)), itself derived from
analyses of abstracts from all statistical software submitted to both rOpenSci
and JOSS. (Several additional analyses and graphical representations of these
raw data are included an [auxiliary github
repository](https://github.com/ropenscilabs/statistical-software).) The primary
nodes that emerge from these empirical analyses (with associated *relative*
sizes in parentheses) are shown in the following table.

```{r top-terms, message = FALSE}
x <- readLines ("scripts/joss-abstracts/categories.Rmd")
x <- x [grep ("[0-9]*\\. \\[", x)]
names <- vapply (x, function (i) {
                     res <- strsplit (i, "\\[\\`") [[1]]
                     strsplit (res, "\\`\\]") [[2]] [1] },
                     character (1),
                     USE.NAMES = FALSE)
terms <- lapply (x, function (i) {
                 res <- strsplit (i, ":\\s") [[1]] [2]
                 res <- strsplit (res, ";\\s") [[1]] [1] # rm "; input"
                 strsplit (res, ",\\s") [[1]]   })
input <- lapply (x, function (i) {
                     res <- strsplit (i, "input: ") [[1]] [2]
                     res <- strsplit (res, ";\\s") [[1]] [1]
                     if (grepl (",\\s", res))
                         res <- strsplit (res, ",\\s") [[1]]
                     return (res)   })
output <- lapply (x, function (i) {
                     res <- strsplit (i, "output: ") [[1]] [2]
                     if (grepl (",\\s", res))
                         res <- strsplit (res, ",\\s") [[1]]
                     return (res)   })
names (terms) <- names (input) <- names (output) <- names
n_abstracts <- length (terms)
terms0 <- terms # used below

terms <- unlist (terms) %>%
    table () %>%
    sort (decreasing = TRUE)
terms <- data.frame (n = seq_along (terms),
                     term = names (terms),
                     proportion = as.integer (terms) / n_abstracts,
                     stringsAsFactors = FALSE)
terms$term [terms$term == "scores"] <- "statistical indices and scores"
n <- max (which (terms$proportion > 0.05))
knitr::kable (head (terms, n = n), digits = c (0, NA, 3), caption = "Most frequent key words from all JOSS abstracts (N = 92) for statistical software. Proportions are scaled *per abstract*, with each abstract generally having multiple key words, and so sum of proportions exceeds one.")
```

```{r collate-categories}
terms$term [terms$term %in% c ("Bayesian", "Monte Carlo")] <- "Bayesian & Monte Carlo"
terms$term [terms$term %in% c ("dimensionality reduction", "feature selection")] <- "dimensionality reduction & feature selection"
terms$term [terms$term %in% c ("regression", "splines", "interpolation")] <- "regression/splines/interpolation"
terms$term [terms$term == "EDA"] <- "Exploratory Data Analysis (EDA)"
terms <- terms %>%
    group_by (term) %>%
    summarise (proportion = sum (proportion)) %>%
    arrange (by = desc (proportion))
```


The top key words and their inter-relationships within the main [network
diagram](https://ropenscilabs.github.io/statistical-software/abstracts/network-terms/index.html)
were used to distinguish the following primary categories representing all
terms which appear in over 5% of all abstracts, along with the two additional
categories of "spatial" and "education". We have excluded the key word
"Estimates" as being too generic to usefully inform standards, and have also
collected a few strongly-connected terms into single categories.

```{r methods-categories}
out <- data.frame (terms [which (terms$proportion > 0.05), ])
out <- out [which (!out$term == "estimates"), ]
out <- rbind (out, terms [which (terms$term == "spatial"), ],
              terms [which (terms$term == "education"), ])
out$comment <- ""
out$comment [out$term == "dimensionality reduction & feature selection"] <-
    "Commonly as a result of ML algorithms"
out$comment [out$term == "regression/splines/interpolation"] <-
    "Including function data analysis"
out$comment [out$term == "probability distributions"] <-
    "Including kernel densities, likelihood estimates and estimators, and sampling routines"
out$comment [out$term == "categorical variables"] <-
    "Including latent variables, and those output from ML algorithms. Note also that method for dimensionality reduction (such as clustering) often transform data to categorical forms."
out$comment [out$term == "Exploratory Data Analysis (EDA)"] <-
    "Including information statistics such as Akaike's criterion, and techniques such as random forests. Often related to workflow software."
out$comment [out$term == "survival"] <-
    "strongly related to EDA, yet differing in being strictly descriptive of software *outputs* whereas EDA may include routines to explore data *inputs* and other pre-output stages of analysis."
out$comment [out$term == "summary statistics"] <-
    "Primarily related in the empirical data to regression and survival analyses, yet clearly a distinct category of its own."
out$comment [out$term == "workflow"] <- "Often related to EDA, and very commonly also to ML."
out$comment [out$term == "model selection"] <-
    "A important intermediate node between such categories as ML, Bayesian and Monte Carlo techniques, visualisation, and EDA."
out$comment [out$term == "statistical indices and scores"] <-
    "Software generally intended to produce specific indices or scores as statistical output"
out$comment [out$term == "spatial"] <-
    "Also an important intermediate node connecting several other nodes, yet defining its own distinct cluster reflecting a distinct area of expertise."

out <- data.frame (n = seq (nrow (out)),
                   term = out$term,
                   proprtion = out$proportion,
                   comment = out$comment,
                   stringsAsFactors = FALSE)

knitr::kable (out, digits = c (0, NA, 3, NA), caption = "Proposed categorisation of statistical software, with corresponding proportions of all JOSS software matching each category")
```


The full network diagram can then be reduced down to these categories only,
with interconnections weighted by all first- and second-order interconnections
between intermediate categories, to give the following, simplified diagram
(in which "scores" denotes "statistical indices and scores"; with the diagram
best inspected by dragging individual nodes to see their connections to
others).

```{r visnetwork-simplified}
nodes <- table (unlist (terms0))
nodes <- data.frame (id = names (nodes),
                     label = names (nodes),
                     value = as.integer (nodes),
                     stringsAsFactors = FALSE)
edges <- lapply (terms0, function (i) {
                     if (length (i) > 1) {
                         res <- sort (i)
                         n <- combn (seq_along (res), 2)
                         cbind (res [n [1, ]],
                                res [n [2, ]])
                     }  })
edges <- do.call (rbind, edges)
edges <- data.frame (from = edges [, 1],
                     to = edges [, 2],
                     stringsAsFactors = FALSE) %>%
    group_by (from, to) %>%
    summarise (width = length (from)) %>%
    ungroup ()
# Remove isolated edges:
# annotation, areal weights, binomial distribution, cubature, gene loci,
# generalized least squares, misspecification, classification, interpolation,
# over-dispersion, integration, random effects, probit model
cl <- graph_from_data_frame (edges) %>%
    clusters ()
out <- names (cl$membership [which (cl$membership != which.max (cl$csize))])
nodes <- nodes [which (!nodes$id %in% out), ]
edges <- edges [which (!(edges$from %in% out | edges$to %in% out)), ]

# manually agglomerate some categories
from <- c ("Bayesian|Monte Carlo", "dimensionality reduction|feature selection",
        "^regression|splines|interpolation")
to <- c ("Bayes/MC", "dimensionality reduction", "regression")
for (i in seq_along (from)) {
    edges$from [grep (from [i], edges$from)] <- to [i]
    edges$to [grep (from [i], edges$to)] <- to [i]
    nodes$id [grep (from [i], nodes$label)] <- to [i]
    nodes$label [grep (from [i], nodes$label)] <- to [i]
}
edges <- edges %>%
    group_by (from, to) %>%
    summarise (width = sum (width)) %>%
    ungroup ()
nodes <- nodes %>%
    group_by (label) %>%
    summarise (value = sum (value)) %>%
    transform (id = label)

g <- graph_from_data_frame (edges)
dg <- distances (g)

# add to edge distances all intermediate distances between O(2) connections
edges2 <- edges
dgnames <- rownames (dg)
for (i in seq (nrow (edges2))) {
    dgfrom <- match (edges2$from [i], rownames (dg))
    dgto <- match (edges$to [i], colnames (dg))
    nbs_from <- dgnames [which (dg [dgfrom, ] < 3)]
    nbs_to <- dgnames [which (dg [, dgto] < 3)]
    nbs <- intersect (nbs_from, nbs_to)
    index <- which (edges$from == edges$from [i] & edges$to %in% nbs)
    edges2$width [i] <- sum (edges$width [index])
}

ns <- nodes [order (nodes$value, decreasing = TRUE), ]
index <- c (which (!grepl ("estimates", ns$label [1:15])),
            grep ("spatial|education", ns$label))
ns <- ns [index, ]
es <- edges2 [which (edges$from %in% ns$label & edges$to %in% ns$label), ]
#es <- es [which (es$from %in% ns$label & es$to %in% ns$label), ]
es <- es [which (!es$from == es$to), ]
es$width <- es$width * 5 / max (es$width)
library (visNetwork)
visNetwork (ns, es)
```

We intend, at least initially, to use these categories to define and guide the
assessment of statistical software. Standards considered under any of the
ensuing categories must be developed with reference to inter-relationships
between categories, and in particular to potential ambiguity within and between
any categorisation. An example of such ambiguity, and of potential difficulties
associated with categorisation, is the category of "network" software which
appropriate describes the
[`grapherator`](https://github.com/jakobbossek/grapherator) package (with
accompanying [JOSS paper](https://joss.theoj.org/papers/10.21105/joss.00528))
which is effectively a distribution generator for data represented in
a particular format that happens to represent a graph; and three JSM
presentations, one on [network-based clustering of high-dimensional
data](https://ww2.amstat.org/meetings/jsm/2018/onlineprogram/AbstractDetails.cfm?abstractid=327171),
one on [community structure in dynamic
networks](https://ww2.amstat.org/meetings/jsm/2018/onlineprogram/AbstractDetails.cfm?abstractid=328764)
and one on [Gaussian graphical
models](https://ww2.amstat.org/meetings/jsm/2018/onlineprogram/AbstractDetails.cfm?abstractid=328764).
Standards derived for network software must accommodate such diversity of
applications, and must accommodate software for which the "network" category
may pertain only to some relatively minor aspect, while the primary algorithms
or routines may not be related to network software in any direct way.

### Bayesian and Monte Carlo Routines

Packages implementing or otherwise relying on Bayesian or Monte Carlo routines
represent form the central "hub" of all categories in the above diagram,
indicating that even though this category is roughly equally common to other
categories, software in this category is more likely to share more other
categories. In other words, this is the leading "hybrid" category within which
standards for all other categories must also be kept in mind. Some examples of
software in this category include:

1. The [`bayestestR`
   package](https://joss.theoj.org/papers/10.21105/joss.01541) "provides tools
   to describe ... posterior distributions"
2. The [`ArviZ` package](https://joss.theoj.org/papers/10.21105/joss.01143) is
   a python package for exploratory analyses of Bayesian models, particularly
   posterior distributions.
3. The [`GammaGompertzCR`
   package](https://joss.theoj.org/papers/10.21105/joss.00216) features
   explicit diagnostics of MCMC convergence statistics.
4. The [`BayesianNetwork`
   package](https://joss.theoj.org/papers/10.21105/joss.00425) is in many ways
   a wrapper package primarily serving a `shiny` app, but also accordingly
   a package in both education and EDA categories.
5. The [`fmcmc` package](https://joss.theoj.org/papers/10.21105/joss.01427) is
   a "classic" MCMC package which directly provides its own implementation, and
   generates its own convergence statistics.
7. The [`rsimsum` package](https://joss.theoj.org/papers/10.21105/joss.00739)
   is a package to "summarise results from Monte Carlo simulation studies".
   Many of the statistics generated by this package may prove useful in
   assessing and comparing Bayesian and Monte Carlo software in general. (See
   also the [`MCMCvis`
   package](https://joss.theoj.org/papers/10.21105/joss.00640), with more of
   a focus on visualisation.)
8. The [`walkr` package](https://joss.theoj.org/papers/10.21105/joss.00061) for
   "MCMC Sampling from Non-Negative Convex Polytopes" is indicative of the
   difficulties of deriving generally applicable assessments of software in
   this category, because MCMC *sampling* relies on fundamentally different
   inputs and outputs than many other MCMC routines.

***Key Considerations*** 

- The extent to which the output of Bayesian routines with uninformative prior inputs can or do
  reflect equivalent frequentist analyses.
- Ways to standardise and compare diagnostic statistics for convergence of MCMC
  routines.

### Dimensionality Reduction and Feature Selection

Many packages either implement or rely upon techniques for dimensionality
reduction or feature selection. One of the primary problems presented by such
techniques is that they are constrained to yield a result independent on any
measure of correctness of accuracy [@estivill-castro_why_2002]. This generally
precludes any assessment of the accuracy or reliability of such routines.
Moreover, dimensionality reduction techniques are often developed for
particular kinds of input data, reducing abilities to compare and contrast
different implementations, as well as to compare them with any notional
reference implementations.


1. [`ivis`](https://joss.theoj.org/papers/10.21105/joss.01596) implements
   a dimensionality reduction technique using a "Siamese Neural Network
   architecture.
2. [`tsfeaturex`](https://joss.theoj.org/papers/10.21105/joss.01279) is
   a package to automate "time series feature extraction," which also provides
   an example of a package for which both input and output data are generally
   incomparable with most other packages in this category.
3. [`iRF`](https://joss.theoj.org/papers/10.21105/joss.01077) is another
   example of a generally incomparable package within this category, here one
   for which the features extracted are the most distinct predictive features
   extracted from repeated iterations of random forest algorithms.
4. [`compboost`](https://joss.theoj.org/papers/10.21105/joss.00967) is
   a package for component-wise gradient boosting which may be sufficient
   general to potentially allow general application to problems addressed by
   several packages in this category. 
10. The [`iml`](https://joss.theoj.org/papers/10.21105/joss.00786) package may
    offer usable functionality for devising general assessments of software
    within this category, through offering a "toolbox for making machine
    learning models interpretable" in a "model agnostic" way.

***Key Considerations***

- There is generally no way to discern the accuracy of reliability of
  dimensionality reduction techniques.
- It is difficult to devise general routines to compare and assess different
  routines in this category, although possible starting points for the
  development of such may be offered by the
  [`compboost`](https://joss.theoj.org/papers/10.21105/joss.00967) and
  [`iml`](https://joss.theoj.org/papers/10.21105/joss.00786) packages.

### Machine Learning

### Regression, Splines, and other Interpolation Methods

### Statistical Indices and Scores

Many packages are designed to provide one or more specific statistical indices,
scores, or summary statistics from some assumed type of input data. Mthodology used to
derive indices or scores may draw on many of the methods or algorithms
considered in the first category above or are often field-specific, arthimetic
calculations.  Such software may
likely be considered within its own category through a singular aim to provide
particular indices or scores, in contrast with more generic "Methods and Algorithms"
software which offers more abstraction or modeled approach.  Some examples include:

1. The
   [`spatialwarnings` package](https://github.com/spatial-ews/spatialwarnings) which provides "early-warning signal of
   ecosystem degradation," where these signals and associated indices are
   highly domain-specific.
1. The
   [`heatsaveR` package](https://github.com/robwschlegel/heatwaveR) which calculates and displays marine heatwaves using
   specific indices established in previously-published literature.
1. The
   [`hhi` package](https://github.com/pdwaggoner/hhi) which calculates and visualizes "Herfindahl-Hirschman Index
   Scores," which are measures of numeric concentration.
1. The
   [`DscoreApp` package](https://github.com/OttaviaE/DscoreApp) which provides an index (the "D-Score") to quantify
   the results of
   [Implicit Association Tests](https://en.wikipedia.org/wiki/Implicit-association_test).
1. The
   [`thurstonianIRT` package](https://github.com/paul-buerkner/thurstonianIRT) (with accompanying
   [JOSS paper](https://joss.theoj.org/papers/10.21105/joss.01662)) for score forced-choice questionnaires using
   ["Item Response Theory"](https://en.wikipedia.org/wiki/Item_response_theory).


 ***Key Considerations***:  Such packages can generally be reviewed for correctness
(or accuracy/precision) in comparison to pseudocode, reference implementations, or reference data sets
and in this way have can be straightforwardly  evaluated. More complex indices and
scores will require many of the considerations in the "methods and algorithms"
category above. In many cases,
the field-specific nature of indices and scores may tightly tie the algorithm
implementation to certain data input formats or workflows common to practitioners.
They may have considerable overlap with workflow packages (below). There is
also the possibility that some indices could be considered "trivial" arithmetic
calculations. We may wish to consider some qualitative standard for additional
utility that such packages would provide.  


### Visualisation

While many may consider software primarily aimed at visualisation to be out of
scope, there are nevertheless cases which may indeed be within scope, notably
including the
[`ggfortify` package](https://github.com/sinhrks/ggfortify) which allows results of statistical tests to be
"automatically" visualised using the
[`ggplot2` package](https://ggplot2.tidyverse.org). The list of "fortified" functions on the packages
[webpage](https://github.com/sinhrks/ggfortify) clearly indicates the very predominantly statistical scope of this
software which is in effect a package for statistical reporting, yet in visual
rather than tabular form. Other examples of visualisation software include:

1. The
   [`modelStudio` package](https://github.com/ModelOriented/modelStudio) (with accompanying
   [ JOSS paper](https://joss.theoj.org/papers/10.21105/joss.01798)), which is also very much a workflow package.
3. The
   [`shinyEFA` package](https://github.com/PsyChiLin/EFAshiny) (with accompanying
   [JOSS paper](https://joss.theoj.org/papers/10.21105/joss.00567)) which provides a, "User-Friendly Shiny Application for
   Exploratory Factor Analysis."
3. The
   [`autoplotly` package](https://github.com/terrytangyuan/autoplotly) (with accompanying
   [JOSS paper](https://joss.theoj.org/papers/10.21105/joss.00657)) which provides, "Automatic Generation of Interactive
   Visualisations for Statistical Results", primarily by porting the output of
   the authors' above-mentioned
   [`ggfortify` package](https://github.com/sinhrks/ggfortify) to
   [`plotly.js`](https://github.com/plotly/plotly.js).

***Key considerations***: The quality or utility visualization techniques can be strongly subjective, but also
may be evaluated using standardized principles if the community can come to a consensus on those principles. Such considerations
may be context-dependent - e.g., the requirements of a diagnostic plot designed to
support model-checking are different from that designed to present raw data or model
results to a new audience.  This implies that the indented purppose of the visualization
should be well-defined.

Whether or not visualization is in-scope, many software packages with other
primary purposes also include functions to visualise output. Visualization will
thus never be *strictly* out of scope.  However one option is not to include
*primarily* visualization packages, or only *statistical* visualization packages
in which visualization is closely tied to another category or purpose. 

Visualisation packages will include numerical or statistical routines for
transforming data from raw form to graphics, which can be evaluated for correctness
or accuracy.

### Probability Distributions

### Wrapper Packages

"Wrapper" packages provide an interface to previously-written software, often in
a different computer language to the original implentation. While this category
is reasonably unambiguous, there may be instances in which a "wrapper"
additionally offers extension beyond original implementations, or in which only
a portion of a package's functionality may be "wrapped."  Rather than internally bundling
or wrapping software, a package may also serve as a wrapper thorugh providing
access to some external interface, such as a web server. Examples of potential
wrapper packages include the following:

1. The
   [`greta` package](https://github.com/greta-dev/greta)
   (with accompanying
   [JOSS article](https://joss.theoj.org/papers/10.21105/joss.01601)) "for writing statistical models and fitting them by MCMC
   and optimisation" provides a wrapper around google's
   [`TensorFlow` library](https://www.tensorflow.org). It is also clearly a workflow package, aiming to
   provide a single, unified workflow for generic machine learning processes
   and analyses.
2. The
   [`nse` package](https://github.com/keblu/nse) (with accompanying
   [JOSS paper](https://joss.theoj.org/papers/10.21105/joss.00172)) which offers "multiple ways to calculate numerical standard
   errors (NSE) of univariate (or multivariate in some cases) time series,"
   through providing a unified interface to several other R packages to provide
   more than 30 NSE estimators. This is an example of a wrapper package which
   does not wrap either internal code or external interfaces, rather it
   effectively "wraps" the algorithms of a collection of R packages.

  ***Key Considerations***:  For many wrapper packages it may not be feasible
for reviewers (or authors) to evaluate the quality or correctness of the wrapped
software, so review could be limited to the interface or added value provided,
or the statistical routines within. 

wrapper packages include the extent of functionality represented by wrapped
code, and the computer language being wrapped. 
- *Internal or External:* Does the software *internally* wrap of bundle
  previously developed routines, or does it provide a wrapper around some
  external service? If the latter, what kind of service (web-based, or some
  other form of remote access)?
- *Language:* For internally-bundled routines, in which computer language
  e the routines written? And how are they bundled? (For R packages: In
  `./src`? In `./inst`? Elsewhere?)
- *Testing:* Does the software test the correctness of the wrapped component?
  Does it rely on tests of the wrapped component elsewhere?
- *Unique Advances:* What unique advances does the software offer beyond
  those offered by the (internally or externally) wrapped software?
  
  ***Proposal:*** The project will only review packages where the primary
statistical functionality is in the main source code developed by the authors,
and not in an external package.  

### Categorical Variables

### Networks

### Statistical Reporting and Exploratory Data Analysis

Many packages aim to simplify and facilitate the reporting of complex
statistical results or exploratory summaries of data. Such reporting commonly involves visualisation, and there
is direct overlap between this and the Visualisation category (below). This roughly breaks out into software
that summarizes and presents _raw_ data, and software that reports complex data derived from statistical routines.
However, this break is often not clean, as raw data exploration may involve an algorithmic or modeling step
(e.g., projection pursuit.). Examples include:

1.  A package rejected by rOpenSci as out-of-scope,
[`gtsummary`](https://github.com/ddsjoberg/gtsummary), which provides, "Presentation-ready data summary and analytic
result tables." Other examples include:
1. The
   [`smartEDA` package](https://github.com/daya6489/SmartEDA) (with accompanying
   [JOSS paper](https://joss.theoj.org/papers/10.21105/joss.01509)) "for automated exploratory data analysis". The package,
   "automatically selects the variables and performs the related descriptive
   statistics. Moreover, it also analyzes the information value, the weight of
   evidence, custom tables, summary statistics, and performs graphical
   techniques for both numeric and categorical variables." This package is
   potentially as much a workflow package as it is a statistical reporting
   package, and illustrates the ambiguity between these two categories.
2. The
   [`modeLLtest` package](https://github.com/ShanaScogin/modeLLtest) (with accompanying
   [JOSS paper](https://joss.theoj.org/papers/10.21105/joss.01542)) is "An R Package for Unbiased Model Comparison using Cross
   Validation." Its main functionality allows different statistical models to
   be compared, likely implying that this represents a kind of meta package.
3. The
   [`insight` package](https://github.com/easystats/insight) (with accompanying
   [JOSS paper](https://joss.theoj.org/papers/10.21105/joss.01412) provides "a unified interface to access information from
   model objects in R," with a strong focus on unified and consistent reporting
   of statistical results.
4. The
   [`arviz` software for python](https://github.com/arviz-devs/arviz) (with accompanying
   [JOSS paper](https://joss.theoj.org/papers/10.21105/joss.01143) provides "a unified library for exploratory analysis of
   Bayesian models in Python."
5. The
   [`iRF` package](https://github.com/sumbose/iRF) (with accompanying
   [JOSS paper](https://joss.theoj.org/papers/10.21105/joss.01077) enables "extracting interactions from random forests", yet
   also focusses primarily on enabling interpretation of random forests through
   reporting on interaction terms.

In addition to potential overlap with the Visualisation category, potential
standards for Statistical Reporting and Meta-Software are likely to overlap to
some degree with the preceding standards for Workflow Software. Checklist items
unique to statistical reporting software might include the following:

- [ ] **Automation** Does the software automate aspects of statistical
  reporting, or of analysis at some sufficiently "meta"-level (such as variable
  or model selection), which previously (in a reference implementation)
  required manual intervention?
- [ ] **General Reporting:** Does the software report on, or otherwise provide
  insight into, statistics or important aspects of data or analytic processes
  which were previously not (directly) accessible using reference
  implementations?
- [ ] **Comparison:** Does the software provide or enable standardisedB
  comparison of inputs, processes, models, or outputs which could previously
  (in reference implementations) only be accessed or compared some comparably
  unstandardised form?
- [ ] **Interpretation:** Does the software facilitate interpretation of
  otherwise abstruse processes or statistical results?
- [ ] **Exploration:** Does the software enable or otherwise guide exploratory
  stages of a statistical workflow?

### Survival Analyses

### Workflow Support

"Workflow" software may not implement particular methods or algorithms,
but rather support tasks around the statsitical process.  In many cases, these
may be generic tasks that apply across methods. These include:

1. Classes (whether explicit or not) for representing or processing input and
   output data;
2. Generic interfaces to multiple statistical methods or algorithms;
3. Homogeneous reporting of the results of a variety of methods or algorithms;
   and
4. Methods to synthesise, visualise, or otherwise collectively report on
   analytic results.

Methods and Algorithms software may only provide a specific interface to
a specific method or algorithm, although it may also be more general and offer
several of the above "workflow" aspects, and so ambiguity may often arise
between these two categories. We note in particular that the "workflow" node in
the
[interactive network diagram](https://ropenscilabs.github.io/statistical-software/abstracts/network-terms)
mentioned above is very strongly connected to the "machine learning" node,
generally reflecting software which attempts to unify varied interfaces to
varied platforms for machine learning.

Among the numerous examples of software in this category are:

1. The
   [`mlr3` package](https://github.com/mlr-org/mlr3) (with accompanying
   [JOSS paper](https://joss.theoj.org/papers/10.21105/joss.01903)), which provides, "A modern object-oriented machine learning
   framework in R."
2. The
   [`fmcmc` package](https://github.com/USCbiostats/fmcmc)
   (with accompanying
   [JOSS paper](https://joss.theoj.org/papers/10.21105/joss.01427)), which provides a unified framework and workflow for
   Markov-Chain Monte Carlo analyses.
3. The
   [`bayestestR` package](https://github.com/easystats/bayestestR) (with accompanying
   [JOSS paper](https://joss.theoj.org/papers/10.21105/joss.01541))
   for "describing effects and their uncertainty, existence and significance
   within the Bayesian framework. While this packages includes its own
   algorithmic implementations, it is primarily intended to aid general
   Bayesian workflows through a unified interface.

Workflows are also commonly required and developed for specific areas of
application, as exemplified by the
[`tabular` package](https://github.com/nfrerebeau/tabula) (with accompanying
[JOSS article](https://joss.theoj.org/papers/10.21105/joss.01821) for "Analysis, Seriation, and visualisation of Archaeological
Count Data".

 ***Key Considerations:*** Workflow packages are popular and add considerable value
and efficiency for users.  One challenge in evaluating such packages is the
importance of API design and potential subjectivity of this.  For instance,
`mlr3` as well as `tidymodels` have similar uses of providing a common interface
to multiple predictive models and tools for automating processes across these
models.  Similar, multiple packages have different approaches for handling MCMC
data.  Each package makes different choices in design and has different priorities,
which may or may not agree with reviewers' opinions or applications.  Despite such
differences, it may be possible to evaluate such packages for *internal* cohesion,
and adherence to a sufficiently clearly stated design goal. Reviewers may be able
to evaluate whether the package provides a _more_ unified workflow or interface
than other packages - this would require a standard of relative improvement over
the field rather than baseline standards.

These packages also often contain numerical routines (cross-validation,
performance scoring, model comparison), that can be evaluated for correctness
or accuracy.  

### Summary Statistics

### Spatial Analyses

### Education

A prominent class of statistical software is *educational* software designed to
teach statistics. Such software many include its own implementations of statistical
methods, and frequently include interactive components.  Many examples of educational statistical software are
listed on the
[CRAN Task View: Teaching Statistics](https://cran.r-project.org/web/views/TeachingStatistics.html). This page also clearly indicates the
likely strong overlap between education and visualisation software. With
specific regard to the educational components of software, the follow checklist
items may be relevant.
A prominent example is the [`LearnBayes` package](https://cran.r-project.org/web/packages/LearnBayes/index.html). 

 ***Key Considerations:*** Correctness of implementation of educational or tutorial
software is important. Evaluation of such software extends considerably beyond correctness,
with heavy emphasis on documentation, interactive interface, and pedagogical soundness
of the software.  These areas enter a very different class of standards.  It is
likely that educational software will very greatly _structurally_, as interaction
may be via graphical or web interfaces, text interaction or some other form.

The [Journal of Open Source Education](https://jose.theoj.org) accepts both educational
software and curricula, and has a peer review system (almost)
identical to [JOSS](https://joss.theoj.org). Educational statistical software reviewed by rOpenSci could thus
potentially be fast-tracked through JOSE reviews just as current
submissions have the opportunity to be fast-tracked through the JOSS review process. 

- *Demand:* Does the software meet a clear demand otherwise absent from
  educational material? If so, how?
- *Audience:* What is the intended audience or user base? (For example,
  is the software intended for direct use by students of statistics, or does it
  provide a tool for educational professionals to use in their own practice?)
- *Algorithms:* What are the unique algorithmic processes implemented by
  the software? In what ways are they easier, simpler, faster, or otherwise
  better than reference implementations (where such exist)?
- *Interactivity:* Is the primary function of the software interactive?
  If so, is the interactivity primarily graphical (for example, web-based),
  text-based, or other?

 **Proposal:** Educational software will not be in-scope for the project. Relevant
methods standards may be able to be adopted by JOSE or similar outlets. 


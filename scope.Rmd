#  (PART) Scope and Standards {-}

```{r startup, echo = FALSE, message = FALSE}
library (dplyr)
library (ggplot2)
library (rvest)
theme_set (theme_minimal ())
```


# Scope {#scope}

One of the primary tasks in extending the current peer review system to the new
domain of statistical software is defining scope, particularly in terms of
determining what might lie either within or beyond scope. Defining scope is
largely an exercise in categorization, requiring the definition of, and
distinction between, categories which will be considered either within or
beyond scope. This endeavour plays a critical role in subsequent development or
expansion of a peer review system because,

1. A categorical definition of scope is critical to determining which kinds of
   software will be admitted;
2. Different categories of software will be subject to different standards, and
   mapping categories on to standards is crucially important for the
   development of a software assessment system.

We consider two main types of categories:

1. Categories defining types of software, referred to as "software
   categories", and determined by computer languages, and forms for packaging
   software in those languages; and
2. Categories defining different types of statistical software, referred to as
   "statistical categories".

## Software categories {#software-categories}

This project was conceived of, and is being conducted, by
[https://ropensci.org](rOpenSci), and so naturally is primarily intended to target the **R**
language, and its well-defined system for structuring software "packages"
(referred to as
[https://cran.r-project.org/doc/manuals/R-exts.html]("extensions") in the original **R** terminology). Other forms of packaging
**R** software may nevertheless be considered within scope, as may other
languages. It is particularly important to note that many **R** packages
distributed by the
[https://cran.r-project.org](Comprehensive R Archive Network (CRAN)) bundle code from a variety of other
languages. The following table summarises statistics for the top ten languages
from all
`r format (nrow (available.packages ()), big.mark = ",")` CRAN packages as of
`r format(Sys.time(), "%a %b %d %Y")`.

While the easiest decision regarding software scope would be to restrict review
submissions to **R** packages only, as is currently practiced by
[https://ropensci.org](rOpenSci), there are compelling reasons to consider extension beyond this
scope. Foremost among potential extensions is the inclusion of python packages,
which would both open up scope to the enormously larger community of python
users, and also enable direct collaboration with the incipient
[https://www.pyopensci.org](pyOpenSci) organisation. Two factors may be usefully noted in this regard:

```{r pypi-projects, echo = FALSE}
u <- "https://pypi.org/"
x <- read_html (u) %>%
    #html_nodes ("div.statistics-bar") %>%
    html_nodes ("p.statistics-bar__statistic") %>%
    html_text ()
pypi_n_projects <- x [grep ("projects", x)] %>%
    gsub ("projects|[[:space:]]", "", .)

cran_n_projects <- format (nrow (available.packages ()), big.mark = ",")

pypi_cran_ratio <- floor (as.integer (gsub (",", "", pypi_n_projects)) /
    as.integer (gsub (",", "", cran_n_projects)))
```

1. The potential number of python packages for statistical analyses is likely
   to be relatively more restricted than relative numbers of **R** packages.
   Taking as indicative presentations at the previous three Joint Statistical
   Meetings (JSMs; 2018-2020), no python packages were referred to in any
   abstrat, while 32 **R** packages were presented, along with two
   meta-platforms for **R** packages. Presentations at the Symposium of Data
   Science and Statistics (SDSS) for 2018-19 similarly including numerous
   presentations of **R** packages, along with presentation of
   [https://altair-viz.github.io](   three)
   [https://github.com/ajboyd2/salmon](   python)
   [https://github.com/dlsun/symbulate](   packages). It may accordingly be expected that potential expansion to
   include python packages will demand relatively very little time or effort
   compared with that devoted to **R** packages as the primary software scope.
2. In spite of the above, the community of python users is enormously greater,
   reflected in the currently `r pypi_n_projects` packages compared with
   `r cran_n_projects` packages on CRAN, or over `r pypi_cran_ratio` times as
   many python packages. Similarly, 41.7% of all respondents to the
   [https://insights.stackoverflow.com/survey/2019](   2019 stackoverflow developer survey) nominated python as their most
   popular language, compared with only 5.8% who nominated **R**.

The relative importance of python is powerfully reflected in temporal trends
from the
[https://insights.stackoverflow.com/survey/2019](stackoverflow developer survey) from the previous three years, with results
shown in the following graphic.

```{r py_v_r, echo = FALSE, message = FALSE}
theme_set (theme_minimal ())
year <- 2017:2019
python_used <- c (31.7, 38.8, 41.7)
r_used <- c (4.4, 6.1, 5.8)
python_loved <- c (62.7, 68.0, 73.1)
r_loved <- c (49.9, 49.4, 51.7)

dat <- rbind (tibble (year = year,
                      proportion = python_used,
                      language = "python-used",
                      context = "used"),
              tibble (year = year,
                      proportion = r_used,
                      language = "R-used",
                      context = "used"),
              tibble (year = year,
                      proportion = python_loved,
                      language = "python-loved",
                      context = "loved"),
              tibble (year = year,
                      proportion = r_loved,
                      language = "R-loved",
                      context = "loved"))

ggplot (dat, aes (x = year, y = proportion, color = language)) +
    geom_line () +
    geom_point () +
    scale_x_continuous (breaks = year)
```

Python is not only more used and more loved than **R**, but both statistics for
python have consistently grown at a faster rate over the past three years as
have equivalent statistics for **R**.

Both languages nevertheless have relative well-defined standards for software
packaging, python via the
[https://packaging.python.org/tutorials/packaging-projects](Python Package Index (pypi)), and **R** via
[https://cran.r-project.org](CRAN). In contrast to CRAN, which runs its own checks on all packages on
a daily basis, there are no automatic checks for
[https://packaging.python.org/tutorials/packaging-projects](pypi) packages, and almost any form of package that minimally conforms to
the standards may be submitted. This much lower effective barrier to entry
likely partially contributes to the far greater numbers of
[https://packaging.python.org/tutorials/packaging-projects](pypi) than
[https://cran.r-project.org](CRAN) packages.

Within **R**, it may be conceivable to consider forms of packaging or bundling
code other than
[https://cran.r-project.org](CRAN)-standard packages. Of particular note here are attempts to develop
a python-like system of
[https://github.com/klmr/modules](modules for **R**).




## Statistical Categories {#statistical-categories}

The major exercise in categorisation which this project must be addressed is to
categorise software into different *statistical* categories. The construction
of statistical categories is best approached as an empirical endeavour, for
which we initially used the
[https://joss.theoj.org](Journal of Open Source Software (JOSS)), which provides a general forum for
peer reviewed software which is likely to be of general use for many domains.

The
[https://joss.theoj.org](JOSS)
conducts its own peer review process, and publishes textual descriptions of
accepted software. Each piece of software then has its own web page on the
journal's site, on which the text is presented as a compiled `.pdf`-format
document, along with links to the open review, as well as to the software
repository. The published document must be included within the software
repository in a file named `paper.md`, which enables automatic extraction and
analysis of these text descriptions of software. Rather than attempt
a comprehensive, and unavoidably subjective, categorization of software, these
textual descriptions were used to identify key words or phrases (hereafter,
"keywords") which encapsulated the purpose, function, or other general
descriptive elements of each piece of software. Each paper generally yielded
multiple keywords. Extracting these from all papers judged to be potentially in
scope allowed for the construction of a network of topics, in which the nodes
were the key words and phrases, and the connections between any pair of nodes
reflected the number of times those two keywords co-occured across all papers.

We extracted all papers accepted and published by JOSS (217 at the time of
writing in early 2020), and manually determined which of these were broadly
statistical, reducing the total to 92. We then read through the contents of
each of these, and recorded as many keywords as possible for each paper. The
resultant network is shown in the following interactive graphic, in which nodes
are scaled by numbers of occurrences, and edges by numbers of co-occurrences.


```{r message = FALSE}
x <- readLines ("stat-software-categories.md")
x <- x [grep ("[0-9]*\\. \\[", x)]
names <- vapply (x, function (i) {
                     res <- strsplit (i, "\\[\\`") [[1]]
                     strsplit (res, "\\`\\]") [[2]] [1] },
                     character (1), USE.NAMES = FALSE)
terms <- lapply (x, function (i) {
                     res <- strsplit (i, ":\\s") [[1]] [2]
                     res <- strsplit (res, ";\\s") [[1]] [1] # rm "; input"
                     strsplit (res, ",\\s") [[1]]   })
input <- lapply (x, function (i) {
                     res <- strsplit (i, "input: ") [[1]] [2]
                     res <- strsplit (res, ";\\s") [[1]] [1]
                     if (grepl (",\\s", res))
                         res <- strsplit (res, ",\\s") [[1]]
                     return (res)   })
output <- lapply (x, function (i) {
                      res <- strsplit (i, "output: ") [[1]] [2]
                      if (grepl (",\\s", res))
                          res <- strsplit (res, ",\\s") [[1]]
                      return (res)   })
names (terms) <- names (input) <- names (output) <- names

# Then convert to `visNetwork` nodes and edges tables:
library (dplyr)
library (igraph)
nodes <- table (unlist (terms))
nodes <- data.frame (id = names (nodes),
                     label = names (nodes),
                     value = as.integer (nodes),
                     stringsAsFactors = FALSE)
edges <- lapply (terms, function (i) {
                     if (length (i) > 1) {
                         res <- sort (i)
                         n <- combn (seq_along (res), 2)
                         cbind (res [n [1, ]], res [n [2, ]]) }
                     })
edges <- do.call (rbind, edges)
edges <- data.frame (from = edges [, 1],
                     to = edges [, 2],
                     stringsAsFactors = FALSE) %>%
    group_by (from, to) %>%
    summarise (width = length (from)) # Remove isolated edges:
      # annotation, areal weights, binomial distribution, cubature, gene loci,
      # generalized least squares, misspecification, classification, interpolation,
      # over-dispersion, integration, random effects, probit model
cl <- graph_from_data_frame (edges) %>%
    clusters ()
out <- names (cl$membership [which (cl$membership != which.max (cl$csize))])
nodes <- nodes [which (!nodes$id %in% out), ]
edges <- edges [which (!(edges$from %in% out | edges$to %in% out)), ]

library (visNetwork)
visNetwork (nodes, edges)
```

Such a network visualization enables immediate identification of more and less
central concepts including, in our case, several that we may not otherwise have
conceived of as having been potentially in scope. We then used this network to
define our set of key "in scope" concepts. This figure also reveals that many
of these keywords are somewhat "lower level" than the kinds of concepts we
might otherwise have used to define scoping categories. For example, keywords
such as "likelihood" or "probability" are not likely to be useful in defining
actual categories of statistical software, yet they turned out to lie at the
centres of relatively well-defined groups of related keywords.

In addition to these data from JOSS, we conducted empirical analyses of both
software and conference presentations from the following sources:

1. All software packages published as articles in both the Journal of
   Statistical Software and the Journal of Open Source Software.
2. All software presented at Joint Statistical Meetings (JSMs) 2018 and 2019,
   or Symposia on Data Science and Statistics (SDSS) 2018, 2019, and 2020.
3. All conference sessions and associated abstracts from JSM 2018 and 2019, and
   SDSS 2018, 2019, and 2020.
4. All CRAN Task Views, and perusal of R packages mentioned therein.

Several descriptions and graphical representations of these raw data are
included in an auxiliary
[https://github.com/ropenscilabs/statistical-software](github repository)
containing a number of descriptions, code scripts, and analyses used to guide
the construction of the following categories. Based on the preceding input
data, we identified the following primary categories of statistical software,
the current intention of which is twofold:

1. To guide decisions of which statistical categories (or sub-components) may
   or may not be in scope; and
2. To relate these statistical categories to corresponding standards.

These categories ought not be considered in any way mutually exclusive, and it
is very likely that any individual piece of software will be described by
multiple categories. Decisions of whether a particular piece of software is
described by any particular category will also generally involve some degree of
ambiguity.

The following categorical descriptions are based primarily on examples which
serve to illustrates the kinds of ambiguities and difficulties likely to arise
in establishing and delineating the respective categories, and accordingly to
guide the construction of standards corresponding to each category.

Explicit standards are currently considered in a separate document. It is
envisioned that an author-provided categorisation will guide the selection of
appropriate standards which can or should be applied to a given piece of
software. The categorisation itself will likely occur via some kind of
checklist, with the possibility of checking multiple potential categories. Each
of the categories described immediately below includes its own checklist
intended to guide the mapping of categories to the explicit standards
considered in the separate document. In all cases, it is likely that authors
will also be asked to add any additional statements within any chosen category
on the unique abilities of the software.


### Methods and Algorithms

This main category encompasses all software which implements statistical
methods and algorithms. See the
[https://github.com/ropenscilabs/statistical-software/blob/master/categories.md]("Raw Data")
section in the auxiliary repository for
details of the myriad of potential sub-categories of statistical methods and
algorithms. There are a number of sub-categories which some may consider
effectively independent, or otherwise beyond the general scope of "Methods and
Algorithms", yet which we consider under this single category because of
perceived inability to provide sufficient categorical distinction. These
include:

1. Network software, either for representing, processing, visualising, or
   analysing networks. Ambiguous examples of such include
   [https://github.com/nvihrs14/tcherry](   `tcherry`) (with accompanying
   [https://joss.theoj.org/papers/10.21105/joss.01480](   JOSS paper));
   [https://github.com/jakobbossek/grapherator](   `grapherator`) (with accompanying
   [https://joss.theoj.org/papers/10.21105/joss.00528](   JOSS paper)) which is effectively a distribution generator for data
   represented in a particular format; and three JSM presentations, one on
   [https://ww2.amstat.org/meetings/jsm/2018/onlineprogram/AbstractDetails.cfm?abstractid=327171](   network-based clustering of high-dimensional data),
   one on
   [https://ww2.amstat.org/meetings/jsm/2018/onlineprogram/AbstractDetails.cfm?abstractid=328764](   community structure in dynamic networks) and one on
   [https://ww2.amstat.org/meetings/jsm/2018/onlineprogram/AbstractDetails.cfm?abstractid=328764](   Gaussian graphical models).
2. Software for analysing categorical or qualitative data, which can not be
   unambiguously distinguished because many methods for dimensionality
   reduction, and particularly clustering methods, effectively transform data
   to categorical forms for subsequent (post-)processing.
3. Spatial software, because spatial statistics are often analogous to
   non-spatial statistics, yet merely differ by being bound two two orthogonal
   dimensions (arbitrary numbers of additional dimensions notwithstanding).

We have also prepared an interactive
[https://ropenscilabs.github.io/statistical-software/abstracts/network-terms](network diagram)
with nodes representing statistical terms scaled by approximate frequencies of
occurrence within JOSS papers, and edges between each pair of nodes scaled
according to numbers of JOSS papers which encompass those two terms or
concepts. This diagram immediately illustrates the entangled nature of
categorical definitions within contemporary statistical software, and provides
a strong argument against attempts to distinguish sub-categories.

We nevertheless need to somehow map Method and Algorithm software onto
corresponding standards, with the following checklist intended to exemplify the
kinds of decisions which will likely need to be made, many in regard to one or
more "reference implementations" which software authors may be requested to
specify:

- [ ] **Efficiency:** Is the software more efficient (faster, simpler, other
  interpretations of "efficient") than reference implementations?
- [ ] **Reproducibility or Reliability:** Does the software reproduce
  sufficiently similar results more frequently than reference implementations
  (or otherwise satisfy similar interpretations of reproducibility)?
- [ ] **Accuracy or Precision:** Is the software demonstrably more accurate or
  precise than reference implementations?
- [ ] **Simplicity of Use:** Is the software simpler to use than reference
  implementations?
- [ ] **Algorithmic Characteristics:** Does the algorithmic implementation
  offer characteristics (such as greater simplicity or sensitivity) superior to
  reference implementations? If so, which?
- [ ] **Convergence:** Does the software provide faster or otherwise better
  convergence properties than reference implementations?
- [ ] **Method Validity:** Does the software overcome demonstrable flaws in
  previous (reference) implementations? If so, how?
- [ ] **Method Appliciability:** Does the software enable a statistical method
  to be applied to a domain in which such application was not previously
  possible?
- [ ] **Automation** Does the software automate aspects of statistical analyses
  which previously (in a reference implementation) required manual intervention?
- [ ] **Input Data:** Does the software "open up" a method to input data
  previously unable to be treated by a particular algorithm or method?
- [ ] **Output Data:** Does the software provide output in forms previously
  unavailable by reference implementations?
- [ ] **Reference Standards:** Are there any reference standards, such as the
  US National Institute of Standards and Technology's
  [https://www.itl.nist.gov/div898/strd](  collection of reference data sets)
  against which the software may be compared? If so, which?

Note that software which is described by any of the following categories may
also directly implement statistical methods or algorithms. Any components of
any software which do so can potentially be assessed against this checklist,
and it may accordingly be useful to have a simple "meta" checkbox for all
software:

- [ ] Does this software directly implement statistical methods or algorithms?

Those components which do so would then be assessed in more detail with regard
to a detailed checklist like the above.

### Workflow

This category encompasses software which is more aimed at supporting common
statistical *workflows* than direct analysis. The primary development effort
for software in this category is presumed *not* to be the implementation of
particular statistical methods or algorithms, rather the algorithmic support of
general statistical workflows. Whereas software in the preceding category may
ultimately yield one or more specific models or statistical values, workflow
software generally provides more than one of the following:

1. Classes (whether explicit or not) for representing or processing input and
   output data;
2. Generic interfaces to multiple statistical methods or algorithms;
3. Homogeneous reporting of the results of a variety of methods or algorithms;
   and
4. Methods to synthesise, visualise, or otherwise collectively report on
   analytic results.

Methods and Algorithms software may only provide a specific interface to
a specific method or algorithm, although it may also be more general and offer
several of the above "workflow" aspects, and so ambiguity may often arise
between these two categories. We note in particular that the "workflow" node in
the
[https://ropenscilabs.github.io/statistical-software/abstracts/network-terms](interactive network diagram)
mentioned above is very strongly connected to the "machine learning" node,
generally reflecting software which attempts to unify varied interfaces to
varied platforms for machine learning.

Among the numerous examples of software in this category are:

1. The
   [https://github.com/mlr-org/mlr3](   `mlr3` package) (with accompanying
   [https://joss.theoj.org/papers/10.21105/joss.01903](   JOSS paper)), which provides, "A modern object-oriented machine learning
   framework in R."
2. The
   [https://github.com/USCbiostats/fmcmc](   `fmcmc` package)
   (with accompanying
   [https://joss.theoj.org/papers/10.21105/joss.01427](   JOSS paper)), which provides a unified framework and workflow for
   Markov-Chain Monte Carlo analyses.
3. The
   [https://github.com/easystats/bayestestR](   `bayestestR` package) (with accompanying
   [https://joss.theoj.org/papers/10.21105/joss.01541](   JOSS paper))
   for "describing effects and their uncertainty, existence and significance
   within the Bayesian framework. While this packages includes its own
   algorithmic implementations, it is primarily intended to aid general
   Bayesian workflows through a unified interface.

Workflows are also commonly required and developed for specific areas of
application, as exemplified by the
[https://github.com/nfrerebeau/tabula](`tabular` package) (with accompanying
[https://joss.theoj.org/papers/10.21105/joss.01821](JOSS article) for "Analysis, Seriation, and visualisation of Archaeological
Count Data".

Relevant standards for workflow software may be guided by consideration of
items such as those in the following checklist, the first four of which in this
case largely reflect the four aspects listed above which workflow software may
generally provide:

- [ ] **Unified Data:** Does the software unify previously disparate forms of
  data in order to enable a consistent workflow?
- [ ] **Unified Interface:** Does the software provide a unified interface to
  several methods which had to previously be accessed using distinct packages
  and/or distinct modes of interface?
- [ ] **Unified Results:** Does the software synthesise results which were
  previously only accessible in individual form?
- [ ] **Unified Workflow:** Does the software enable a coherent workflow which
  was not previously possible via any single package?
- [ ] **Educational Workflow** Does the software primarily serve an educational
  purpose, and do so through enabling a structured workflow that aids
  understanding of statistical or analytic processes?


### Statistical Reporting and Meta-Software

Many packages aim to simplify and facilitate the reporting of complex
statistical results. Such reporting commonly involves visualisation, and there
is direct overlap between this and the Visualisation category. Examples of this
category include one package rejected by rOpenSci as out-of-scope,
[https://github.com/ddsjoberg/gtsummary](`gtsummary`), which provides, "Presentation-ready data summary and analytic
result tables." Other examples include:

1. The
   [https://github.com/daya6489/SmartEDA](   `smartEDA` package) (with accompanying
   [https://joss.theoj.org/papers/10.21105/joss.01509](   JOSS paper)) "for automated exploratory data analysis". The package,
   "automatically selects the variables and performs the related descriptive
   statistics. Moreover, it also analyzes the information value, the weight of
   evidence, custom tables, summary statistics, and performs graphical
   techniques for both numeric and categorical variables." This package is
   potentially as much a workflow package as it is a statistical reporting
   package, and illustrates the ambiguity between these two categories.
2. The
   [https://github.com/ShanaScogin/modeLLtest](   `modeLLtest` package) (with accompanying
   [https://joss.theoj.org/papers/10.21105/joss.01542](   JOSS paper)) is "An R Package for Unbiased Model Comparison using Cross
   Validation." Its main functionality allows different statistical models to
   be compared, likely implying that this represents a kind of meta package.
3. The
   [https://github.com/easystats/insight](   `insight` package) (with accompanying
   [https://joss.theoj.org/papers/10.21105/joss.01412](   JOSS paper) provides "a unified interface to access information from
   model objects in R," with a strong focus on unified and consistent reporting
   of statistical results.
4. The
   [https://github.com/arviz-devs/arviz](   `arviz` software for python) (with accompanying
   [https://joss.theoj.org/papers/10.21105/joss.01143](   JOSS paper) provides "a unified library for exploratory analysis of
   Bayesian models in Python."
5. The
   [https://github.com/sumbose/iRF](   `iRF` package) (with accompanying
   [https://joss.theoj.org/papers/10.21105/joss.01077](   JOSS paper) enables "extracting interactions from random forests", yet
   also focusses primarily on enabling interpretation of random forests through
   reporting on interaction terms.

In addition to potential overlap with the Visualisation category, potential
standards for Statistical Reporting and Meta-Software are likely to overlap to
some degree with the preceding standards for Workflow Software. Checklist items
unique to statistical reporting software might include the following:

- [ ] **Automation** Does the software automate aspects of statistical
  reporting, or of analysis at some sufficiently "meta"-level (such as variable
  or model selection), which previously (in a reference implementation)
  required manual intervention?
- [ ] **General Reporting:** Does the software report on, or otherwise provide
  insight into, statistics or important aspects of data or analytic processes
  which were previously not (directly) accessible using reference
  implementations?
- [ ] **Comparison:** Does the software provide or enable standardised
  comparison of inputs, processes, models, or outputs which could previously
  (in reference implementations) only be accessed or compared some comparably
  unstandardised form?
- [ ] **Interpretation:** Does the software facilitate interpretation of
  otherwise abstruse processes or statistical results?
- [ ] **Exploration:** Does the software enable or otherwise guide exploratory
  stages of a statistical workflow?

### Education

Software for the purposes of education may be considered in scope. A prominent
example of this category is the
[https://cran.r-project.org/web/packages/LearnBayes/index.html](`LearnBayes` package), which provides functions for learning Bayesian
inference, and includes many of its own implementations. This category may be
of particular interest or relevance because of a potentially direct connection
with the
[https://jose.theoj.org](Journal of Open Source Education), which has a peer review system (almost)
identical to the
[https://joss.theoj.org](JOSS). Educational statistical software reviewed by rOpenSci could thus
potentially be fast-tracked through JOSE reviews just as current
(non-statistical) submissions have the opportunity to be fast-tracked through
the JOSS review process. Many examples of educational statistical software are
listed on the
[https://cran.r-project.org/web/views/TeachingStatistics.html](CRAN Task View: Teaching Statistics). This page also clearly indicates the
likely strong overlap between education and visualisation software. With
specific regard to the educational components of software, the follow checklist
items may be relevant.

- [ ] **Demand:** Does the software meet a clear demand otherwise absent from
  educational material? If so, how?
- [ ] **Audience:** What is the intended audience or user base? (For example,
  is the software intended for direct use by students of statistics, or does it
  provide a tool for educational professionals to use in their own practice?)
- [ ] **Algorithms:** What are the unique algorithmic processes implemented by
  the software? In what ways are they easier, simpler, faster, or otherwise
  better than reference implementations (where such exist)?
- [ ] **Interactivity:** Is the primary function of the software interactive?
  If so, is the interactivity primarily graphical (for example, web-based),
  text-based, or other?

### Visualisation

While many may consider software primarily aimed at visualisation to be out of
scope, there are nevertheless cases which may indeed be within scope, notably
including the
[https://github.com/sinhrks/ggfortify](`ggfortify` package) which allows results of statistical tests to be
"automatically" visualised using the
[https://ggplot2.tidyverse.org](`ggplot2` package). The list of "fortified" functions on the packages
[https://github.com/sinhrks/ggfortify](webpage) clearly indicates the very predominantly statistical scope of this
software which is in effect a package for statistical reporting, yet in visual
rather than tabular form. Other examples of visualisation software include:

1. The
   [https://github.com/ModelOriented/modelStudio](   `modelStudio` package) (with accompanying
   [https://joss.theoj.org/papers/10.21105/joss.01798](   JOSS paper)), which is also very much a workflow package.
3. The
   [https://github.com/PsyChiLin/EFAshiny](   `shinyEFA` package) (with accompanying
   [https://joss.theoj.org/papers/10.21105/joss.00567](   JOSS paper)) which provides a, "User-Friendly Shiny Application for
   Exploratory Factor Analysis."
3. The
   [https://github.com/terrytangyuan/autoplotly](   `autoplotly` package) (with accompanying
   [https://joss.theoj.org/papers/10.21105/joss.00657](   JOSS paper)) which provides, "Automatic Generation of Interactive
   Visualisations for Statistical Results", primarily by porting the output of
   the authors' above-mentioned
   [https://github.com/sinhrks/ggfortify](   `ggfortify` package) to
   [https://github.com/plotly/plotly.js](   `plotly.js`).

Many software packages also include functions to visualise output, and this
category can likely not be considered out of scope in any absolute sense
because there must exist a grey zone with regard to the relative amount of code
devoted to visualisation routines implemented by a software package for it to
be considered a "visualisation" package. Decisions of scope are likely better
made with regard to whether the primary purpose of the visualisation is indeed
*statistical*, or perhaps by insisting that visualisation software must also
represent at least one of the other primary categories considered here.
A checklist for visualisation software may consist of items such as the
following.

- [ ] **Purpose:** Does the software primarily serve a visualisation purpose?
  If so, is that purpose primary or secondary?
- [ ] **Statistical Importance:** Do the visualisations provide important
  abilities in presenting or interpreting *statistical* data, models, results,
  or processes not (readily) provided through other software?
- [ ] **Algorithms and Methods:** Are the visualisations generated by internal
  implementations of *statistical* algorithms or methods?
- [ ] **Education:** Are the visualisation capabilities of the software
  (primarily) intended to serve an educational purpose?
- [ ] **Wrapper:** Is the package a wrapper around visualisation software not
  previously (directly) accessible for the statistical analyses enabled by the
  package?


### Wrapper Packages

Wrapper packages provide an interface to previously-written software, often in
a different computer language to the original implentation. While this category
is reasonably unambiguous, there may be instances in which a "wrapper"
additionally offers extension beyond original implementations, or in which only
a portion of a package's functionality may be "wrapped." Issues to consider for
wrapper packages include the extent of functionality represented by wrapped
code, and the computer language being wrapped. Rather than internally bundling
or wrapping software, a package may also serve as a wrapper thorugh providing
access to some external interface, such as a web server. Examples of potential
wrapper packages include the following:


1. The
   [https://github.com/greta-dev/greta](   `greta` package)
   (with accompanying
   [https://joss.theoj.org/papers/10.21105/joss.01601](   JOSS article)) "for writing statistical models and fitting them by MCMC
   and optimisation" provides a wrapper around google's
   [https://www.tensorflow.org](   `TensorFlow` library). It is also clearly a workflow package, aiming to
   provide a single, unified workflow for generic machine learning processes
   and analyses.
2. The
   [https://github.com/keblu/nse](   `nse` package) (with accompanying
   [https://joss.theoj.org/papers/10.21105/joss.00172](   JOSS paper)) which offers "multiple ways to calculate numerical standard
   errors (NSE) of univariate (or multivariate in some cases) time series,"
   through providing a unified interface to several other R packages to provide
   more than 30 NSE estimators. This is an example of a wrapper package which
   does not wrap either internal code or external interfaces, rather it
   effectively "wraps" the algorithms of a collection of R packages.

The following checklist items may be relevant in considering wrapper software.

- [ ] **Internal or External:** Does the software *internally* wrap of bundle
  previously developed routines, or does it provide a wrapper around some
  external service? If the latter, what kind of service (web-based, or some
  other form of remote access)?
- [ ] **Language:** For internally-bundled routines, in which computer language
  are the routines written? And how are they bundled? (For R packages: In
  `./src`? In `./inst`? Elsewhere?)
- [ ] **Testing:** Please provide references, links, or other material relating
  to or describing how the wrapped software has been tested, and describe how
  such *prior* tests have been integrated within current package structure.
- [ ] **Unique Advances:** What unique advances does the software offer beyond
  those offered by the (internally or externally) wrapped software?

### Statistical Indices and Scores

Many packages are designed to provide one or more specific statistical indices
or scores from some assumed type of input data. Even though methodology used to
derive indices or scores may draw on many of the methods or algorithms
considered in the first category above, and detailed below, such software may
likely be considered within its own category through a singular aim to provide
particular indices or scores, in contrast with generic "Methods and Algorithms"
software which offers some degree of abstraction in terms of either input or
output data, or both. Examples include,


1. The
   [https://github.com/spatial-ews/spatialwarnings](   `spatialwarnings` package) which provides "early-warning signal of
   ecosystem degradation," where these signals and associated indices are
   highly domain-specific.
1. The
   [https://github.com/robwschlegel/heatwaveR](   `heatsaveR` package) which calculates and displays marine heatwaves using
   specific indices established in previously-published literature.
1. The
   [https://github.com/pdwaggoner/hhi](   `hhi` package) which calculates and visualizes "Herfindahl-Hirschman Index
   Scores," which are measures of numeric concentration.
1. The
   [https://github.com/OttaviaE/DscoreApp](   `DscoreApp` package) which provides an index (the "D-Score") to quantify
   the results of
   [https://en.wikipedia.org/wiki/Implicit-association_test](   Implicit Association Tests).
1. The
   [https://github.com/paul-buerkner/thurstonianIRT](   `thurstonianIRT` package) (with accompanying
   [https://joss.theoj.org/papers/10.21105/joss.01662](   JOSS paper)) for score forced-choice questionnaires using
   [https://en.wikipedia.org/wiki/Item_response_theory](   "Item Response Theory").

The following checklist items may be relevant in considering software developed
to calculate particular indices or scores.

- [ ] **Uniqueness:** Are there any implementations to calculate the indices or
  scores in other computing languages? If so, what are they?
- [ ] **Intended Audience:** Who is likely to gain through an ability to
  calculate such indices or scores?
- [ ] **Accuracy or Precision:** Is it possible to confirm the accuracy or
  precision of the algorithmic implementation?
- [ ] **Performance:** Does the software offer superior performance over
  equivalent ways of calculating analogous scores or indices? If so, how may
  such performance gains be assessed?
- [ ] **Utility:** How laborious would the calculation of the given indices or
  scores be with alternative software? And what would be the risks of doing so?
- [ ] **Reproducibility and Other Potential Advantages:** Is the ability of the
  software to calculate the given indices or scores likely to enhance the
  ability of others to reproduce either a general workflow, or previous
  results, or both?


### Additional Applied Categories

There will likely be a host of additional categories of software developed for
particular applied domains. One distinguishing feature of such software appears
to be the use of custom-developed classes or equivalent representations for
input (and often output) data.



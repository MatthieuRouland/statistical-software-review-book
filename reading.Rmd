#  Some Light Reading: An Annotated Bibliography {#reading}

Note that the following annotated bibliography is automatically generated from
all entries containing "Note" fields in the [zotero library accompanying this
project](https://www.zotero.org/groups/2416765/statistical-software?). Please
feel free to add any additional entries you may see fit to this open library.
You'll need to first use [zotero](https://zotero.org) to get a local copy of
the library, then manually add a "Note" to any new entries and they'll
automatically appear here the next time this page is updated. We have not yet
automated connection between zotero and the rendering of this page, so you'll
need to somehow let us (Mark Padgham or Noam Ross) know if you've modified the
library in any way, and we'll ensure the entries are updated.

<!---
Lots to put here, but some in addition to stuff in current document:

-   <https://www.alexpghayes.com/blog/type-stable-estimation/>,
    <https://www.alexpghayes.com/blog/testing-statistical-software/>

-   <https://tidymodels.github.io/model-implementation-principles/>

-   <https://github.com/pharmaR/white_paper>

-   <https://github.com/coreinfrastructure/best-practices-badge/blob/master/doc/criteria.md>
-->

## RWsearch: Lazy Search in R Packages, Task Views, CRAN, the Web. All-in-One Download

[Kiener  (<https://orcid.org/0000-0002-0505-9920>), Patrice  ( 2020-02-15 )  ](https://CRAN.R-project.org/package=RWsearch)

**Abstract** Search by keywords in R packages, task views, CRAN, the web and display the results in console, txt, html or pdf pages. Download the whole documentation (html index, pdf manual, vignettes, source code, etc) with a single instruction. Visualize the package dependencies. Several functions for task view maintenance and exploration of CRAN archive. Quick links to more than 70 web search engines. Lazy evaluation of non-standard content is available throughout the package and eases the use of many functions.

**NOTES**

May be useful for it's ability to download and search package documentation.

## coreinfrastructure/best-practices-badge

[Core Infrastructure  (  )  ](https://github.com/coreinfrastructure/best-practices-badge)

**Abstract** üèÜCore Infrastructure Initiative Best Practices Badge - coreinfrastructure/best-practices-badge

**NOTES**

A definitive reference for general standards in open-source software. That said, it is a github-based document, and many of the standards directly describe github-based attributes and workflows.


¬†


Sections considered are:

- Basic
- Website
  - License
  - Documentation
  - Other
- Must use https
  - Must provide for discussion
  - Must be in English
  - Change Control
- Public version control repo
  - Unique version numbering
  - Semantic versioning
  - Release notes
  - Reporting
- Bug reporting
  - Vulnerability reporting
  - Quality
- Working build system
  - Automated test suite (cover most branches, input fields, and functionality)
  - Continuous integration
  - New functionality testing
  - Warning flags / linter
  - Security
  - Analysis
- Static code analysis (with links to lots of language-specific tools)
  - Dynamic code analysis (with links to lots of language-specific tools)
  - Use of memory check tools, or memory-safe languages



## Reconciling modern machine learning practice and the bias-variance trade-off

[Belkin, Mikhail; Hsu, Daniel; Ma, Siyuan; and Mandal, Soumik  ( 2019-09-10 )  arXiv:1812.11118 [cs, stat]](http://arxiv.org/abs/1812.11118)

**Abstract** Breakthroughs in machine learning are rapidly changing science and society, yet our fundamental understanding of this technology has lagged far behind. Indeed, one of the central tenets of the field, the bias-variance trade-off, appears to be at odds with the observed behavior of methods used in the modern machine learning practice. The bias-variance trade-off implies that a model should balance under-fitting and over-fitting: rich enough to express underlying structure in data, simple enough to avoid fitting spurious patterns. However, in the modern practice, very rich models such as neural networks are trained to exactly fit (i.e., interpolate) the data. Classically, such models would be considered over-fit, and yet they often obtain high accuracy on test data. This apparent contradiction has raised questions about the mathematical foundations of machine learning and their relevance to practitioners. In this paper, we reconcile the classical understanding and the modern practice within a unified performance curve. This "double descent" curve subsumes the textbook U-shaped bias-variance trade-off curve by showing how increasing model capacity beyond the point of interpolation results in improved performance. We provide evidence for the existence and ubiquity of double descent for a wide spectrum of models and datasets, and we posit a mechanism for its emergence. This connection between the performance and the structure of machine learning models delineates the limits of classical analyses, and has implications for both the theory and practice of machine learning.

**NOTES**

A very good reference for the ubiquity of over-fitting in machine learning algorithms, which is nevertheless mostly dedicated to demonstrating a very practical approach to overcoming over-fitting.

## Software Verification and Validation: An Engineering and Scientific Approach

[Fisher, Marcus S.  ( 2007 )  ](https://www.springer.com/gp/book/9780387327259)

**Abstract** The World is lacking an in-depth technical book describing the methods and techniques used to provide confidence in our system software. Not only is the U.S. government more focused on software safety in today's market, but private industry and academia are as well. The methods and techniques that provide such confidence are commonly called software verification and validation. Software Verification and Validation: An Engineering and Scientific Approach, a professional book, fills the critical need for an in-depth technical reference providing the methods and techniques for building and maintaining confidence in many varieties of system software. The intent of this volume is to help develop reliable answers to such critical questions as: 1) Are we building the right software for the need? 2) Are we building the software right? Software Verification and Validation: An Engineering and Scientific Approach is structured for research scientists and practitioners in industry. This book is also suitable as a secondary textbook for advanced-level students in computer science and engineering.

**NOTES**

The first section on an "Engineering Approach to Verification and Validation" outlines 4 phases of:


- Requirements Analysis
- Design Analysis
- Code Analysis
- Test Analysis


They then present a matrix with aspects of these four phases assessed in terms of the IEEE1012-1998 standard, which defines four "Software Integrity Levels" (High, Major, Moderate, Low, mapping on to functions being Critical, Important, Able to be worked around, Merely annoying if absent or failing).


### Risk


They define risk as Likelihood times Consequence, and describe in detail a workflow for risk assessment and management of


- Identify
- Analyze
- Plan
- Track
- Control


Their suggested process is simply but likely quite effective, and consists mainly of coarsely scoring Consequences in order to quanity risk, and then constructing decision trees with four possible exit points of


1. Research
2. Watch
3. Accept; or
4. Mitigate

## Software Quality Approaches: Testing, Verification, and Validation: Software Best Practice 1

[Haug, Michael; Olsen, Eric W; and Consolini, Luisa  ( 2001 )  ](https://doi.org/10.1007/978-3-642-56612-7)

**Abstract** This book is a result of the European Experience Exchange (EUREX) project sponsored by the European Systems and Software Initiative for Software Best Practice in Europe. The EUREX project analyzed the industrial and economic impact and the common aspects and differences between and among more than 300 Software Process Improvement Experiments sponsored by the EU. The current volume offers a variety of perspectives on software quality issues resulting from that analysis, including testing, verification and validation. This area represents one of the "great unknowns" in software development in the sense that many organisations, especially small and medium-sized enterprises, have no purposeful process addressing these issues. As a result, this book is particularly meaningful for software practitioners in such enterprises, including both developers and line managers.

**NOTES**

A huge EU project, drawn from results of a host of "Process Improvement Experiments" (PIEs), all of which were controlled experiments built on a generic model, and involved manipulating some standard baseline practice and analysing effects.

### Levels of Testing


Testing Level | Done by | Description
--- | --- | ---
Unit Testing | Developers | to test each module separately in order to verify that it executes as specified without any programming error
Functional Testing | Application Domain Specialist | to test each function separately in order to verify that functional requirements are implemented as stated in the Functional Specification document. Formal test cases are defined and executed; errors are recorded and test results analysed
Product Testing | Application Domain Specialist | to verify proper execution of the whole product and to evaluate the external product interface; testing procedures are the same as in Functional Testing
System Testing | Application Domain Specialist | to verify proper execution of the whole product in the target environment (hardware and software); volume and stress tests are executed and performance is evaluated
Installation Testing | Developers | to verify the install ability of the product. To verify the product packaging and documentation (readable, correct and complete)
Validation Testing | Customer | to obtain an independent assessment of the quality level from someone who will act as a first user (customer view)


Page 39: "The main weakness of the testing process currently used to test software products can be identified in the areas of
- test execution
- test documentation management
- measurement framework
- testing organisation and the cultural environment"


Static Analyses are, "totally manual and time consuming, and for these reasons they should only be performed for critical parts of the code." - but p119 onward summaries one of the PIEs which looked at manual code inspection, and found it to be only marginally less cost efficient, and actually quite effective.


### Best Practices


(Starting from p202; and with their emphasis on financial aspects removed here)


- Develop new skills
- Formalize vertification process
- Automation (introduced "carefully but inevitably")
- Measure results

## Software testing automation tips: 50 things automation engineers should know

[Alpaev, Gennadiy  ( 2017 )  ](http://www.books24x7.com/marc.asp?bookid=135445)

**Abstract** Quickly access 50 tips for software test engineers using automated methods. The tips point to practices that save time and increase the accuracy and reliability of automated test techniques. Techniques that play well during demos of testing tools often are not the optimal techniques to apply on a running project. This book highlights those differences, helping you apply techniques that are repeatable and callable in professionally run software development projects. Emphasis is placed on creating tests that, while automated, are easily adapted as the software under construction evolves toward its final form. Techniques in the book are arranged into five categories: scripting, testing, the environment, running and logging of tests, and reviewing of the results. Every automation engineer sooner or later will face similar issues to the ones covered in these categories, and you will benefit from the simple and clear answers provided in this book. While the focus of the book is on the use of automated tools, the tips are not specific to any one vendor solution. The tips cover general issues that are faced no matter the specific tool, and are broadly applicable, often even to manual testing efforts. What You'll Learn: Employ best-practices in automated test design Write test scripts that will easily be understood by others Choose the proper environment for running automated tests Avoid techniques that demo well, but do not scale in practice Manage tests effectively, including testing of test scripts themselves Know when to go beyond automation to employ manual methods instead.

**NOTES**

The 50 Things are:


### Scripting


1. Do Not Use Record &amp; Play in Real Projects
2. Do Not use Pauses
3. Provide Exit by Timeout for Loops
4. Do Not Consider Test Automation as Full-Fledged Development
5. Do Not Write Bulky Code
6. Verify All Options of Logical Conditions
7. Use Coding Standards
8. Use Static Code Analyzers
9. Add an Element of Randomness to Scripts
10. Do Not Perform Blind Clicks Against Nonstandard Controls
11. Learn and Use Standard Libraries
12. Avoid Copy and Paste
13. Do Not Use try...catch with an Empty catch Block
14. Separate Code from Data
15. Learn How to Debug
16. Do Not Write Code for the Future
17. Leave the Code Better Than It Was
18. Choose a Proper Language for GUI Tests
19. Remember to Declare and Initialize Variables


### Testing


20. Do Not Duplicate Tested Application Functionality in the Scripts
21. Each Test Should Be Independent
22. What Should Not Be Automated?
23. Ask the Developers for Help
24. Cloud Testing
25. Introduce Automation for Corner Cases
26. The Difference Between Error and Warning
27. Use the Appropriate Methodologies
28. Verification of Individual Bugs
29. Make a Pilot Project Before Writing Real Tests


### Environment


30. Choose a Proper Set of Tools for Your Needs
31. Do Not Automatically Register Bugs from Scripts
32. Do Not Chase After a ‚ÄúGreen Build‚Äù in the Prejudice of Quality
33. Learn the Tool You Work With
34. Make Use of Version Control Systems
35. Avoid Custom Forms
36. Simplify Everything You Can
37. Automate Any Routine


### Running, Logging, Verifying


38. Run Scripts as Often as Possible
39. Perform an Automatic Restart of Failed Tests
40. A Disabled Test Should Be Provided with a Comment
41. Errors in Logs Should Be Informative
42. Make a Screenshot in Case of Error
43. Check the Accuracy of Tests Before Adding Them to the
Regular Run
44. Avoid Comparing Images


### Reviewing


45. Write Tests That Even Non-Automation Engineers Can Understand
46. Avoid Unneeded Optimization
47. Review Someone Else‚Äôs Code Regularly
48. Participate in Forums and Discussions
49. Perform Refactoring
50. Remove Tests That Provide Minimal Benefit

## Introduction to software testing

[Ammann, Paul; and Offutt, Jeff  ( 2017 )  ]()

**Abstract** 

**NOTES**

### Introduction to Software Testing


Paul Ammann &amp; Jeff Offutt (2017)


Starts with useful definitions (p34) of:


- *Verification* "The process of determining whether the
products of a phase of the software development process fulfill the requirements established during the previous phase."
- *Validation* "The process of evaluating software at theend of software development to ensure compliance with intended usage."


They also start with five levels of testing, as defined by Beizer (*Software Testing Techniques*, 1990), noting that the first was not considered worthy of having an actual number, of:


- *Level 0* There is no difference between testing and debugging.
- *Level 1* The purpose of testing is to show correctness.
- *Level 2* The purpose of testing is to show that the software does not work.
- *Level 3* The purpose of testing is not to prove anything specific, but to reduce the risk of using the software.
- *Level 4* Testing is a mental discipline that helps all IT professionals develop higher- quality software.


They do that to emphasise that we should all aim for *Level 4*.


### Model-Driven Test Design (Chapter 2)


&gt; Designers are more efficient and effective if they can raise their level of abstraction. (p48)


They develop a "Reachability, Infection, Propagation, and Revealability (RIPR)" model, but that's just putting nouns of the self-evident process of fault discovery. They repeat the V-Model of Mili &amp; Tchier (2015). Most of the rest of the chapter then develops a semi-formal description building towards describing testing as traversing all edges of a program graph.


Then goes on to derive a really impressive formalisation of graph and path coverage methods. This chapter seems to provide a pretty definitive reference to this stuff.


Chapter 7 (p208-&gt;) briefly discusses the problems of graph/path testing approaches and object-oriented languages. This is a problem because the edges in an "inheritance hierarchy are not execution flow at all" (p209)


The first 8 chapters cover generating tests from input spaces, from graphs, and from logical expressions. This can be used a standard reference on all of these points. They then move on (Chapter 9) to *Syntax-Based Testing*, for which they use a slightly-modified regex kind of system for expressing conditions and expectations, all considered a kind of [BNF Grammar](https://en.wikipedia.org/wiki/Backus%E2%80%93Naur_form)


### Writing Test Plans (Chapter 11)


This has some really useful guideance, distinguishing a *Master Test Plan* from a *Level Test Plan* (a sub-unit for a particular program level). They provide an example template for the latter which consists of:


1. Introduction
- Document Identifier
- Scope
- References
- Level in the Overall Sequence
- Test Classes and Overall Test Conditions
2. Details for this Level of Test Plan
- Test Items and Identifiers
- Test Traceability Matrix
- Features to be Tested
- Features Not to be Tested
- Approach
- Item Pass/Fail Criteria
- Suspension Criteria and Resumption Requirements
- Test Deliverables
3. Test Management
- Planned Activities and Tasks; Test Management
- Environment and Infrastructure
- Responsibilities and Authority
- ...
4. General 
- Quality Assurance Procedures
- Metrics
- Test Coverage
- Glossary
- Document Change Procedures and History

## Software testing: concepts and operations

[Mili, Ali  ( 2015 )  ]()

**Abstract** 

**NOTES**

### Software Testing: Concepts &amp; Operations


Ali Mili &amp; Fairouz Tchier (2015)


&gt; Many fields of science and engineering (such as bioinformatics, medical informatics, weather forecasting, and modeling and simulation) are so dependent on software that they can almost be considered as mere applications of software engineering (p5)


Software requires both:


&gt; 1. Process controls, ensuring that software products are developed and evolved according to certified, mature processes.
&gt; 2. Product controls, ensuring that software products meet quality standards commensurate with their application domain requirement


Describes some problems facing modern software development (p9), including "Absence of Reuse Practice", and lack of standard software architecture. Neither of these are applicable to R packages.


Then goes on to describe (p11) "The Absence of Automation", and "Limited Quality Control", both of which are definitely still applicable.


----


### Software Quality Attributes


Chapter 2 starts with a list of attribute categories:


- Functional Attributes
- Operational Attributes (not relevant to R packages)
- Usability Attributes
- Business Attributes (not relevant to R packages)
- Strutural Attributes


Each of these is then examined in turn.


### Functional Attributes


These are useful diagnosed as either boolean or statisrtical attributes. The two boolean attributes are


1. Correctness
2. Robustness - does the software behave reasonably *outside* the domain of specification


Robustness is binary because it implies logical correctness.


### Useability Attributes


These are sub-categorised into:


1. Ease of Use
2. Ease of Learning
3. Customizability
4. Calibrability
5. Interoperability


### Structural Attributes


These include:


1. Design Integrity
2. Modularity (including "cohesion" and "coupling")
3. Testability
4. Adaptability


----


### A Software Testing Lifecycle


The chapter carefully uses the indefinite article, but usefully identifies the following four generic phases:


1. *Preparing a Test Environment* 
2. *Generating Test Data*, which can be done:
- *Functionally* by fitting tests to software functions
- *Structurally* by fitting tests to all components of the software (both functional and other); or
- *Randomly*
3. *Generating an Oracle* or a test reporter.
4. *Generating a Termination Condition*, which is essentially guided by the decision of whether tests are *integration tests*, *regression tests*, *reliability tests*, or something else. This phases is likely less relevant to R packages than the others.
5. *Producing a Test Driver*, such as the decision to use `testthat`.
6. *Executing the Test*, and
7. *Analyzing Test Outcome*


They present a flow diagram of this general procedure on p31, but this is not really applicable to R packages, for which the equivalent flowchart is by definition always linear.


**Question** Is there a need to generate some kind of equivalent flow chart that could or might be used to guide the development and writing of tests in R?


The chapter concludes with "The V-Model of Software Testing," which is actually quite instructive, particularly in clearly linking unit testing with programming, while linking integration testing with software design. (Same model is given in Ammann &amp; Offutt, 2017)


### Specification


Chapter 4 on Specification is interesting, even if not directly relevant. Notable points are the formalisation of specification in terms of:


1. *Formality* The specification must be represented in such a way as to describe precisely what functional behavior is required.
2. *Abstraction*: The specification must describe what requirements the software product must satisfy, not how to satisfy them. In other words, it must focus on *what* candidate programs must do rather than *how* they must do it, the latter being the prerogative of the designer.


They also clarify that a specification must be both *complete* and *minimal*. Most of the chapter is then an introduction to set notation and theory, in order to derive a formal language for specification. Cool, but way beyond what would be possible within R packages.


### A Software Testing Taxonomy (Chapter 7)


The chapter describes how difficult such a taxonomy is, and then approaches the task by attempting to derive a minimal orthogonal basis for describing testing.


### Primary Attributes of Testing


1. *Scale* - module, subsystem, system
2. *Goal* - finding faults, counting faults, reliability, certification, ...
3. *Property* - functional properties, performance properties, stress, robustness, ...
4. *Method* - how to generate test data


They use these to derive an "early map" between test goals and methods:


Goal | Structural | Functional | Random
--- | --- | --- | ---
Finding and removing faults | X | |
Proving absence of faults | | X | 
Estimating Frequency of failure | | | X
Ensuring Infrequency of Failure | | | X
Estimating Fault Density | | | X


The chapter then dives in to each of those in more detail, and derives a series of analogous sub-tables disssecting relationships between types and attributes of tests. All really good stuff, but not directly related to R packages.


Probability distributions for test data are briefly considered on p154-155, but only very cursorily. Section 9.2 on Test Data Generation from Tabular Expressions (p171-&gt;) is quite interesting. Typical input data are partitioned into cross-tabulated (that is, 2-D) groups, to allow consideration and examination of the range of likely inputs in terms of inter-relationships between input variables.


Chapter 9 on *Structural Criteria* for testing may be useful at a later stage in providing a very formal specification of structural tracing of code for testing purposes, including the meaning of *paths*, *path conditions*, and *control flow*.


### Test Outcome Analysis (Chapter 13)


Starts by comparing "Concrete" with "Symbolic" testing (p283), which primarily serves to indicate that only weak conclusions about actual program behaviour can be drawn by traditional "concrete" approaches, whereas much more difficult "symbolic" approaches can actually generate strong and formal conclusions about program behaviour. Then goes on to examine "Stocahstic" testing, which focusses on "likely properties rather than logically provable properties" (p284).


### Metrics for Software Testing (Chapter 14)


1. *Fault Proneness*
2. *Fault Detectability*
3. *Error Detectability*
4. *Error Maskability*
5. *Failure Avoidance*
6. *Failure Tolerance*


### Fault Proneness


Under the first of these they then develop formal definitions (p315-&gt;) for
1. *Cyclomatic Complexity* (number of edges - number of nodes + 2.
2. *Volume* If $N$ is the number of lexemes of a program (operators, operands, symbols, constants, etc.), and $n$ is the number of unique lexemes, then the program's *volume* is $V=N\times log_2(n)$. The value $log_2(n)$ is the number of binary decisions one has to make to select each of the $n$ symbols, and so $V$ is effort expended in selecting $N$ symbols from a vocabulary of size $n$.


Both of these are measures of complexity, and greater complexity is associated with greater *fault proneness*.


### Fault Detectability


Three measures are considered of:


1. *The P-Measure*, which is the probability that $n$ tests generated at random do not cause the program to fail, and so is $P=1-(1-\theta)^n$.
2. *The E-Measure*, which is the expeted number of failures from $n$ random tests, and so is $E=n\theta$; and
3. *The F-Measure*, which is either the expected mean or median expected number of tests prior to test failure.


These all depend on $\theta$, which is the probability of failure of a test dependent on a randomly generated initial state. They illustrate a formal calculation of $\theta$, before concluding that this value must of course be estimated from practical measurements - but they do not suggest how to do this.

## The art of software testing

[Myers, Glenford J; Badgett, Tom; and Sandler, Corey  ( 2012 )  ]()

**Abstract** 

**NOTES**

### The Art of Software Testing


Glenford Myers, Corey Sandler, Tom Badgett (2011)


Glenford Myers originally published "The Art of Software Testing" in 1979.


### Initial self-test


The book begins with a very clever self-test: Design a program to read three integers from an input, representing the lengths of the sides of a triangle. The program must determine whether the triangle is equilateral, isosceles (two sides equal), or scalene (all sides different). They then list 13 test cases that the program should pass, including:


1. Is triangle valid? (1, 2, 3 is not valid)
2. Do test cases include values of 0?
3. Do test cases include those where A+B &lt; C (1, 2, 4, for example)?
4. Test case of (0, 0, 0)?
5. Test case for non-integer values?
6. Test case for wrong number of values?


... exhaustive testing is difficult!


### Summary


Most of the book is either pretty straightforward, or overly specific to be able to be directly transferred or translated to open source software.


### How Many Users do You Need?


This really interesting question is posed on p148, where they claim that previous research has suggested the really useful formula,
$E = 100\times (1 - (1 - L) ^ n)$,
where


- $E$ = percent of error found;
- $n$ = number of testers
- $L$ = percent of useability problems found by users.


They illustrate with an example of $L=31$%, which gives $E=100(1-0.69^n)$, which exceeds 2/3 at $n\ge3$ and 90% for $n\ge6.3$.

## Medical device software verification, validation and compliance

[Vogel, David A  ( 2011 )  ](http://site.ebrary.com/id/10436227)

**Abstract** Here's the first book written specifically to help medical device and software engineers, QA and compliance professionals, and corporate business managers better understand and implement critical verification and validation processes for medical device software. Offering you a much broader, higher-level picture than other books in this field, this book helps you think critically about software validation -- to build confidence in your software's safety and effectiveness. The book presents validation activities for each phase of the development lifecycle and shows: why these activities are important and add value; how to undertake them; and what outputs need to be created to document the validation process. From software embedded within medical devices, to software that performs as a medical device itself, this comprehensive book explains how properly handled validation throughout the development lifecycle can help bring medical devices to completion sooner, at higher quality, and in compliance with regulations. Additionally, an entire part of the book is devoted to the validation of software that automates any part of a manufacturer's quality system and is regulated by 21 CFR 820.70(i). DVD Included! Contains a collection of FDA regulations and guidance documents related to software in the medical device industry, valuable sample forms and templates, and supplemental figures that support key topics covered in the book.

**NOTES**


&gt; A common problem in software validation is miscommunication because of loosely defined terminology. (p4)


&gt; we can avoid philosophical debates about the rightness and wrongness of terminology. All that is needed is to keep a running glossary or table of definitions for the intended meaning of the terminology in the context in which it is currently used. As long as the usage of the terminology is consistent, clear communication is established. (p6)


### Software Life Cycles


&gt; Software validation takes place within the environment of an established software life cycle. The software life cycle contains software engineering tasks and documentation necessary to support the software validation effort. In addition, the software life cycle contains specific verification and validation tasks that are appropriate for the intended use of the software.


(p60; from the FDA's "General Principles of Software Validation")


Components of a software life cycle model include:
- Quality Planning
- System Requirements Definition
- Detailed Software Requirements Specification
- Software Design Specification
- Construction or Coding
- Testing
- Installation
- Operation and Support
- Maintenance
- Retirement


### Metrics


&gt; There is no such thing as a perfect metric when it comes to software, but some metrics are better than no metrics. Project metrics are probably of most value... (p62)


### Validation and Verification


Starts by attempting to define what validation *is not*, starting with the myth that,


&gt; validation is simply a lot of testing at the end of the product development process to ensure there are no defects in the product. This is wrong for two reasons. As we will see, it is clear that validation is equally focused on preventing defects from being designed into the software, and on detecting and correcting those defects that do manifest themselves in the software. A validation program that is focused only on testing misses the opportunity to perfect the design and implementation before handing it over to the test group. (p75)


&gt; A validation program that depends on testing alone for a defect-free device is depending on perfection in testing. Unfortunately, test design, development, and implementation suffer from the same imperfections as software design, development, and implementation. (p76)


A Venn diagram is presented on p77 (Fig. 6.2), with validation completely containing the subset f verification, and testing overlapping both of these.


&gt; Verification activities are those which verify that individual design inputs have been properly addressed at each phase of the life cycle... Verification includes some testing. Testing verifies that individual requirements have been implemented correctly. However, reviews are also verification activities, but are not tests. (p76)


The FDA defines validation as including:


- Change Management
- Requirements
- Traceability
- Defect Resolution
- Regression Testing
- Planning
- Risk Analysis
- Evaluations
- Testing
- Intended Use / User Needs
- Design Reviews


&gt; much of what is included in software validation is nothing more than good software engineering practices. (p78)


### FDA Defintion of Validation


&gt; Confirmation by examination and provision of objective evidence that software specifications conform to user needs and intended uses, and that the particular requirements implemented through software can be consistently fulfilled.


(p79) They go on to explain that the first of these conditions is, "accomplished through reviews, planning, user testing, and other activities," while the second is, "accomplished by detailed testing of each specification or requirement of the software. This is also known as verification testing."


&gt; In devices of any complexity, analyzing the software in isolation from the rest of the device‚Äôs subsystems can give a distorted view of the effectiveness of the software. This is true of validation, and as we will see in upcoming chapters, it is also true of risk management, defect management, and configuration management. (p79)


### FDA Definition of Verification


&gt; Software verification provides objective evidence that the design outputs of a particular phase of the software development life cycle meet all of the specified requirements for that phase... A conclusion that software is validated is highly dependent upon ... [the] verification tasks performed at each stage of the software development life cycle. (p83)


### Software (Development) Life Cycles


A good general overview in Chapter 5, which primarily serves to assert that one ought never presume the universal applicability of a single model. Each bit of software is different, and may requires a different development life cycle model.


### Software Reviews


Part of Chapter 9, starting on p143, with reviews considered explicitly from p167 on. A few categories of reviews are identified, but these are quite specific to medical devices, rather than general. Nevertheless, the following useful insights are offered. p174 states that the "IEEE Standard for Software Reviews" introduced the useful concepts of "entry and exit criteria" defining when a review should start and end. This standard (1028-2008) is summarised in [wikipedia](https://en.wikipedia.org/wiki/Software_review#IEEE_1028_generic_process_for_formal_reviews)


The discussion of how to formulate software requirements from around p220 onwards could be very useful as a guideline for how to translate software behaviour expectations into human language.


### Software Testing


A nice definition is given on p256:


&gt; Software testing: An activity in which a system, subsystem, or individual unit of software is executed under specified conditions. The expected results are anticipated from requirements or designs. The actual results are observed and recorded. An assessment is made as to whether the actual results are acceptably equivalent to the anticipated results.


The GPSV defines the following distinct levels of testing (also p256):


&gt; In order to provide a thorough and rigorous examination of a software product, development testing is typically organized into levels. As an example, a software product‚Äôs testing can be organized into unit, integration, and system levels of testing.
&gt; 1) Unit (module or component) level testing focuses on the early examination of sub-program functionality and ensures that functionality not visible at the system level is examined by testing. Unit testing ensures that quality software units are furnished for integration into the finished software product.
&gt; 2) Integration level testing focuses on the transfer of data and control across a program‚Äôs internal and external interfaces. External interfaces are those with other software (including operating system software), system hardware, and the users and can be described as communications links.
&gt; 3) System level testing demonstrates that all specified functionality exists and that the software product is trustworthy. This testing verifies the as-built program‚Äôs functionality and performance with respect to the requirements for the software product as exhibited on the specified operating platform(s).‚Äù


### Coverage


&gt; Certain unit-level test tools can automatically calculate coverage metrics based on the tests designed. These metrics are generally metrics for how much of the source code is exercised. They are not indicative of how well the tests are designed. These metrics are a measure of the breadth of coverage, not the depth or quality of coverage. (p263)


Plus further discussion of other metrics on p267, and a suggestion that proportion of documentation comments to code provides a useful metric.

## Datasheets for Datasets

[Gebru, Timnit; Morgenstern, Jamie; Vecchione, Briana; Vaughan, Jennifer Wortman; Wallach, Hanna; Daume√© III, Hal; and Crawford, Kate  ( 2019-04-14 )  arXiv:1803.09010 [cs]](http://arxiv.org/abs/1803.09010)

**Abstract** Currently there is no standard way to identify how a dataset was created, and what characteristics, motivations, and potential skews it represents. To begin to address this issue, we propose the concept of a datasheet for datasets, a short document to accompany public datasets, commercial APIs, and pretrained models. The goal of this proposal is to enable better communication between dataset creators and users, and help the AI community move toward greater transparency and accountability. By analogy, in computer hardware, it has become industry standard to accompany everything from the simplest components (e.g., resistors), to the most complex microprocessor chips, with datasheets detailing standard operating characteristics, test results, recommended usage, and other information. We outline some of the questions a datasheet for datasets should answer. These questions focus on when, where, and how the training data was gathered, its recommended use cases, and, in the case of human-centric datasets, information regarding the subjects' demographics and consent as applicable. We develop prototypes of datasheets for two well-known datasets: Labeled Faces in The Wild and the Pang \& Lee Polarity Dataset.

**NOTES**
Comment: Working Paper, comments are encouraged

## Design and code inspections to reduce errors in program development

[Fagan, M. E.  ( 1976 )  IBM Systems Journal (3): 182-211](http://extras.springer.com/2002/978-3-642-59413-7/4/rom/pdf/Fagan_hist.pdf)

**Abstract** We can summarize the discussion of design and code inspections and process control in developing programs as follows: 1. Describe the program development process in terms of operations, and define exit criteria which must be satisfied for completion of each operation. 2. Separate the objectives of the inspection process operations to keep the inspection team focused on one objective at a time: Operation Overview Preparation Inspection Rework Follow-up Objective Communications/education Education Find errors Fix errors Ensure all fixes are applied correctly 3. Classify errors by type, and rank frequency of occurrence of types. Identify which types to spend most time looking for in the inspection. 4. Describe how to look for presence of error types. 5. Analyze inspection results and use for constant process improvement (until process averages are reached and then use for process control).



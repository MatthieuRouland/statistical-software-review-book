#  (PART) Package Development, Submission, and Maintenance {-}


# Package Development {#pkgdev}

Package developers are expected to follow the three sub-sections described in
this chapter, starting with a determination of whether a package is likely to
be considered within the scope of our review system. We then describe
our automated assessment tools, which packages are expected to pass, followed
by a description of our system for which developers are expected to self-assess
their software against all applicable standards. As described at the outset,
this book is also the definitive reference for all standards, as provided in
the [dedicated chapter](#standards).

## Scope

The first important task prior to submitting a package is to estimate whether
a package is likely to be considered within our scope for statistical software.
As described in the [Overview](#overview-scope), packages are generally
considered in scope if they fit one or more of the [categories listed
there](#overview-categories), and in the corresponding standards.

Developers are encouraged to contact us at any point prior to, or during,
development, to ask about whether a package might be in scope, or which
categories it might fit within. Categorisation of packages may not always be
straightforward, and we particularly encourage developers who are unsure about
whether a package belongs in a particular category or not to contact us for
discussion. An initial judgement of whether or not a package belongs in
a particular category may be gained by examining the respective standards. Any
package for which a large number of standards from a particular category may be
considered applicable (regardless of whether or not they would actually be
checked) is likely to fit within that category.

## The [`autotest` package](https://github.com/ropenscilabs/autotest) {#pkgdev-autotest}

We have developed an automated assessment tool,
[`autotest`](https://github.com/ropenscilabs/autotest), which all packages are
expected to pass in order to be accepted for submission. The package implements
a form of "mutation testing," by examining the types of all input parameters,
implementing type-specific mutations, and examining the response of each
function in a package to all such mutations. This kind of mutation testing is
a very effective way to uncover any unexpected behaviour which developers
themselves might not necessarily pre-empt.

Full documentation is provided on the [package
website](https://ropenscilabs.github.io/autotest/), with the current sub-section
describing how the output of
[`autotest`](https://ropenscilabs.github.io/autotest/) is expected to be used
in a review process. In order to under *how* the results of 
[`autotest`](https://ropenscilabs.github.io/autotest/) are used in our review
process, it is clearly to describe the general process *in reverse*, beginning
with the final step of making the results available through uploading to
GitHub.

The result of applying the [`autotest_package()`
function](https://ropenscilabs.github.io/autotest/reference/autotest_package.html)
to a package is contained in the object returned from this function, which is
a package-specific class of [`tibble`](https://tibble.tidyverse.org). Prior to
review, you'll need to upload this result according to the following steps:

1. Save a local version of the output of
   [`autotest_package()`](https://ropenscilabs.github.io/autotest/reference/autotest_package.html)
   as `.Rds`, using
   [`saveRDS()`](https://stat.ethz.ch/R-manual/R-devel/library/base/html/readRDS.html).
2. Make a GitHub release of your package called "pre-review" (see
   [here](https://docs.github.com/en/github/administering-a-repository/managing-releases-in-a-repository)
   for more information).
3. Upload the saved object by dragging-and-dropping a locally-saved version
   into the main box for describing the release. (You can easily edit a release
   at any subsequent time if you forget to upload the data when you create the
   release).

That will make your [`autotest`](https://ropenscilabs.github.io/autotest/)
results available for our editors to examine. You can also use rOpenSci's own
[`piggyback` package](https://github.com/ropensci/piggyback) to manage these
tasks entirely within R.

Prior to uploading, however, you will need to determine which tests you
consider not to be applicable to your package, and turn those off at the
appropriate levels (package, function, or parameter level), as described in the 
[documentation](https://ropenscilabs.github.io/autotest). Each test that is
switched off then has a value in the `type` column of `no_test`.


## Assessment Against Standards

Developers are expected to align their package as best they can with both
General Standards and the category-specific standards relevant to that package.
This alignment with standards requires copies of all standards, which can be
obtained in markdown-formatted checklist format from the [project's
R package](https://github.com/ropenscilabs/statistical-software-review) (by
using the [`rssr_standards_checklist()`
function](https://ropenscilabs.github.io/statistical-software-review/reference/rssr_standards_checklist.html)).

### Automated Completion of Standards via [`autotest`](https://github.com/ropenscilabs/autotest)

The first step to completing standards is to use the
[`autotest`](https://github.com/ropenscilabs/autotest) output to automatically
complete the corresponding standards. This can be done by the function XXX of
the [project's
R package](https://github.com/ropenscilabs/statistical-software-review). That
function will nevertheless generally only complete a small portion of all
relevant standards, with the rest needing to be manually completed.

### Manual Completion of Standards

---

## Extra Stuff

A strong focus of this project will be the development of tools to assess
software, both generally and for statistical software specifically. One
important aim is to develop tools able to be used by software authors to assess
their own software. Such self-assessment, along with associated standardised
reporting of results, will ease pre-submission enquiries both on the part of
submitting authors, and editors responsible for assessing such enquiries.
Standardised reporting is considered in the [submission
phase](#submission-phase), while the remainder of the present sub-section
considers tools for self-assessment.

Current rOpenSci practices expect authors to assess their software using our
package standards, then editors perform automated assessment using 
[`goodpractice`](https://github.com/mangothecat/goodpractice), a package which
runs [`R CMD
check`](https://cran.r-project.org/doc/manuals/R-exts.html#Checking-packages)
as well as other tests including calculating test coverage, [linting](https://en.wikipedia.org/wiki/Lint_(software)),
and checking for some common coding anti-patterns.
The [`PharmaR` project's](https://pharmar.org) [`riskmetric`](https://github.com/pharmaR/riskmetric) package performs similar functions as well as providing more development-based metrics and providing a more extensible framework.
While authors commonly use the `goodpractice` assessment tool, demonstrated self-assessment is not
currently required at submission.

We anticipate developing a system for self-evaluation of
software, both in generic form able to be widely applied to software regardless
of category, as well as specific tools for statistical software. Many of these
generic assessments have been listed at the end of the preceding
[Framework](#overview), while tools specific to statistical software have been
considered in the preceding [Scope](#scope) chapter.

***Key Considerations*** The primary consideration here is actually one of the
primary considerations of the entire project, which is what sort of tools might
best be developed? It will be possible to develop extremely sophisticated
tools, but at the expense of compromising progress in other important aspects
of the project. Perhaps more than any other aspect of this project, answering
this question will require maintaining a keen awareness of the compromises
necessary to successfully deliver all desired project outcomes.

***Proposal***

1. Authors will be expected to run automated self-assessments prior to submission.
2. We develop a tool for general assessment of software and
   reporting of analytics, with several modules extending to specific
   assessment of statistical software.
3. We simultaneously develop a lightweight infrastructure to enable such
   assessment and reporting to be provided as an online service, so that authors
   can run assessment in the same environment as it will be run at submission.


## General Software Metrics

The following is an incomplete list of the kinds of metrics commonly used to
evaluate software in general, and which might provide useful for assessing
statistical software in the present project.

-   Code structure
    -   Cyclomatic complexity
    -   Codebase size
    -   Function size / number
    -   Numbers of external calls within functions
    -   Numbers and proportions of Exported / non exported functions
    -   Code consistency
    -   Dynamic metrics derived from function call networks or similar
        -   Network-based metrics both for entire packages, for individual
            functions, and derived from analyses of test coverage
        -   Functional overlap with other packages

-   Documentation metrics:
    -   Numbers of documentation lines per function
    -   Proportion of documentation to code lines
    -   Presence of examples
    -   Vignettes

-   Data documentation metrics
    -   Intended and/or permitted kinds of input data
    -   Nature of output data
    -   Description of data used in tests

-   Meta structure
    -   Dependencies
    -   Reverse dependencies

-   Meta metrics
    -   License (type, availability, compatibility)
    -   Version control?
    -   Availability of website
    -   Availability of source code (beyond CRAN or similar)
    -   Community:
        -   Software downloads and usage statistics
        -   Numbers of active contributors
        -   Numbers or rates of issues reported
    -   Maintenance:
        -   Rate/Numbers of releases
        -   Rate of response to reported issues
        -   Last commit
        -   Commit rate
    -   stars (for github, gitlab, or equivalent for other platforms)
    -   forks

-   Extent of testing
    -   Code coverage
    -   Examples and their coverage
    -   Range of inputs tested

-   Nature of testing
    -   Testing beyond `R CMD check`?
    -   Testing beyond concrete testing?

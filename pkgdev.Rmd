#  (PART) Package Development, Submission, and Maintenance {-}


# Package Development {#pkgdev}

## Scope

The first important task prior to submitting a package is to estimate whether
a package is likely to be considered within our scope for statistical software.
As described in the [Overview](#overview-scope), packages are generally
considered in scope if they fit one or more of the [categories listed
there](#overview-categories), and in the corresponding standards.

Developers are encouraged to contact us at any point prior to, or during,
development, to ask about whether a package might be in scope, or which
categories it might fit within. Categorisation of packages may not always be
straightforward, and we particularly encourage developers who are unsure about
whether a package belongs in a particular category or not to contact us for
discussion. An initial judgement of whether or not a package belongs in
a particular category may be gained by examining the respective standards. Any
package for which a large number of standards from a particular category may be
considered applicable (regardless of whether or not they would actually be
checked) is likely to fit within that category.

## Automated Assessment

Packages are expected to be extensively self-assessed prior to initial
submission enquiries. In particular, packages are expected to pass our
automated testing tools, as described in this section, and developers are
expected to have self-assessed their software against all applicable standards,
as described in the following section.

Our automated assessment tool,
[`autotest`](https://github.com/ropenscilabs/autotest), implements a form of
"mutation testing," by examining the types of all input parameters, implement
type-specific mutations, and examining the response of each function in
a package to all such mutations. Full documentation is provided on the [package
webpages](https://ropenscilabs.github.io/autotest). All statistical software
packages to be submitted for review will be expected to yield clean results
from [`autotest`](https://github.com/ropenscilabs/autotest). As with standards,
there may be tests which developers deem not to be applicable. In such cases
...

**TODO: Implement this ability in autotest**

Note that 
[`autotest`](https://github.com/ropenscilabs/autotest) works by extracting all
code from the `examples` section of each functions' documentation.
A pre-requisite for successful 
[`autotest`](https://github.com/ropenscilabs/autotest)-ing is thus that all
functions must have example code. This can be confirmed by running
[`autotest_package()`](https://ropenscilabs.github.io/autotest/reference/autotest_package.html)
with `test = FALSE`. This returns a list of all tests without actually running
them, along with diagnoses of whether or not functions have sufficient
documentation.

An additional tool developed for this project is
[`packgraph`](https://github.com/ropenscilabs/packgraph), which generates an
interactive visualization of the function call network of a package. The output
of this tool forms part of the initial information provided to package
reviewers, and guides them to understand how the various functions of a package
are inter-related.

## Assessment Against Standards

Developers are expected to align their package as best they can with both
General Standards and the category-specific standards relevant to that package.
This alignment with standards requires copies of all standards, which can be
obtained in markdown-formatted checklist format from the [project's
R package](https://github.com/ropenscilabs/statistical-software-review) (by
using the [`rssr_standards_checklist()`
function](https://ropenscilabs.github.io/statistical-software-review/reference/rssr_standards_checklist.html)).

### Automated Completion of Standards via [`autotest`](https://github.com/ropenscilabs/autotest)

The first step to completing standards is to use the
[`autotest`](https://github.com/ropenscilabs/autotest) output to automatically
complete the corresponding standards. This can be done by the function XXX of
the [project's
R package](https://github.com/ropenscilabs/statistical-software-review). That
function will nevertheless generally only complete a small portion of all
relevant standards, with the rest needing to be manually completed.

### Manual Completion of Standards

---

## Extra Stuff

A strong focus of this project will be the development of tools to assess
software, both generally and for statistical software specifically. One
important aim is to develop tools able to be used by software authors to assess
their own software. Such self-assessment, along with associated standardised
reporting of results, will ease pre-submission enquiries both on the part of
submitting authors, and editors responsible for assessing such enquiries.
Standardised reporting is considered in the [submission
phase](#submission-phase), while the remainder of the present sub-section
considers tools for self-assessment.

Current rOpenSci practices expect authors to assess their software using our
package standards, then editors perform automated assessment using 
[`goodpractice`](https://github.com/mangothecat/goodpractice), a package which
runs [`R CMD
check`](https://cran.r-project.org/doc/manuals/R-exts.html#Checking-packages)
as well as other tests including calculating test coverage, [linting](https://en.wikipedia.org/wiki/Lint_(software)),
and checking for some common coding anti-patterns.
The [`PharmaR` project's](https://pharmar.org) [`riskmetric`](https://github.com/pharmaR/riskmetric) package performs similar functions as well as providing more development-based metrics and providing a more extensible framework.
While authors commonly use the `goodpractice` assessment tool, demonstrated self-assessment is not
currently required at submission.

We anticipate developing a system for self-evaluation of
software, both in generic form able to be widely applied to software regardless
of category, as well as specific tools for statistical software. Many of these
generic assessments have been listed at the end of the preceding
[Framework](#overview), while tools specific to statistical software have been
considered in the preceding [Scope](#scope) chapter.

***Key Considerations*** The primary consideration here is actually one of the
primary considerations of the entire project, which is what sort of tools might
best be developed? It will be possible to develop extremely sophisticated
tools, but at the expense of compromising progress in other important aspects
of the project. Perhaps more than any other aspect of this project, answering
this question will require maintaining a keen awareness of the compromises
necessary to successfully deliver all desired project outcomes.

***Proposal***

1. Authors will be expected to run automated self-assessments prior to submission.
2. We develop a tool for general assessment of software and
   reporting of analytics, with several modules extending to specific
   assessment of statistical software.
3. We simultaneously develop a lightweight infrastructure to enable such
   assessment and reporting to be provided as an online service, so that authors
   can run assessment in the same environment as it will be run at submission.


## General Software Metrics

The following is an incomplete list of the kinds of metrics commonly used to
evaluate software in general, and which might provide useful for assessing
statistical software in the present project.

-   Code structure
    -   Cyclomatic complexity
    -   Codebase size
    -   Function size / number
    -   Numbers of external calls within functions
    -   Numbers and proportions of Exported / non exported functions
    -   Code consistency
    -   Dynamic metrics derived from function call networks or similar
        -   Network-based metrics both for entire packages, for individual
            functions, and derived from analyses of test coverage
        -   Functional overlap with other packages

-   Documentation metrics:
    -   Numbers of documentation lines per function
    -   Proportion of documentation to code lines
    -   Presence of examples
    -   Vignettes

-   Data documentation metrics
    -   Intended and/or permitted kinds of input data
    -   Nature of output data
    -   Description of data used in tests

-   Meta structure
    -   Dependencies
    -   Reverse dependencies

-   Meta metrics
    -   License (type, availability, compatibility)
    -   Version control?
    -   Availability of website
    -   Availability of source code (beyond CRAN or similar)
    -   Community:
        -   Software downloads and usage statistics
        -   Numbers of active contributors
        -   Numbers or rates of issues reported
    -   Maintenance:
        -   Rate/Numbers of releases
        -   Rate of response to reported issues
        -   Last commit
        -   Commit rate
    -   stars (for github, gitlab, or equivalent for other platforms)
    -   forks

-   Extent of testing
    -   Code coverage
    -   Examples and their coverage
    -   Range of inputs tested

-   Nature of testing
    -   Testing beyond `R CMD check`?
    -   Testing beyond concrete testing?

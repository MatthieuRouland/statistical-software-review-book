
# Standards {#standards}

This Chapter is divided between:

-   "*General Standards*" which may be applied to all software considered within
    this project, irrespective of how it may be categorized under the times of
    categories of statistical software listed above; and

-   "*Specific Standards*" which apply to different degrees to statistical
    software depending on the software category.

It is likely that standards developed under the first category may subsequently
be deemed to be genuinely *Statistical Standards* yet which are applicable
across all categories, and it may also be likely that the development of
category-specific standards reveals aspects which are common across all
categories, and which may subsequently be deemed general standards. We
accordingly anticipate a degree of fluidity between these two broad categories.

There is also a necessary relationship between the Standards described here,
and processes of Assessment described below in [Chapter 8](#assessment). We
consider the latter to describe concrete *and generally quantitative* aspects
of *post hoc* software assessment, while the present Standards provides guides
and benchmarks against which to *prospectively* compare software during
development. As this entire document is intended to serve as the defining
reference for our Standards, that term may in turn be interpreted to reflect
this entire document, with the current section explicitly describing aspects of
Standards not covered elsewhere.

As described above, we anticipate the ongoing development of this current
document to employ a versioning system, with software reviewed and hosted under
the system mandated to flag the latest version of these standards to which it
complies.

## Other Standards

Among the noteworthy instances of software standards which might be adapted for
our purposes, and in addition to entries in our [*Annotated
Bibliography*](#reading), the following are particularly relevant:

1. The [Core Infrastructure Initiative's Best Practices
   Badge](https://bestpractices.coreinfrastructure.org/en), which is granted to
   software meeting an extensive list of
   [criteria](https://github.com/coreinfrastructure/best-practices-badge/blob/master/doc/criteria.md).
   This list of criteria provides a singularly useful reference for software
   standards.
2. The [Software Sustainability Institute](https://www.software.ac.uk/)'s
   [*Software Evaulation
   Guide*](https://www.software.ac.uk/resources/guides-everything/software-evaluation-guide),
   in particular their guide to [*Criteria-based software
   evaluation*](http://software.ac.uk/sites/default/files/SSI-SoftwareEvaluationCriteria.pdf),
   which considers two primary categories of *Usability* and *Sustainability
   and Maintainability*, each of which is divided into numerous sub-categories.
   The guide identifies numerous concrete criteria for each sub-category,
   explicitly detailed below in order to provide an example of the kind of
   standards that might be adapted and developed for application to the present
   project.
3. The [*Transparent Statistics
   Guidelines*](https://transparentstats.github.io/guidelines/), by the "HCI
   (Human Computer Interaction) Working Group". While currently only in its
   beginning phases, that document aims to provide concrete guidance on
   "transparent statistical communication." If its development continues, it is
   likely to provide useful guidelines on best practices for how statistical
   software produces and reports results.
4. The more technical considerations of the [Object Management
   Group](https://www.omg.org/index.htm)'s [*Automated Source Code CISQ
   Maintainability Measure*](https://www.omg.org/spec/ASCMM/) (where CISQ
   refers to the [*Consortium for IT Software
   Quality*](https://www.it-cisq.org/)). This guide describes a number of
   measures which can be automatically extracted and used to quantify the
   maintainability of source code. None of these measures are not already
   considered in one or both of the preceding two documents, but the
   identification of measures particularly amenable to automated assessment
   provides a particularly useful reference.

There is also rOpenSci's guide on [package development, maintenance, and peer
review](https://devguide.ropensci.org/), which provides standards of this type
for R packages, primarily within its first chapter. Another notable example is
the [tidyverse design guide](https://principles.tidyverse.org/), and the
section on [Conventions for R Modeling
Pacakges](https://tidymodels.github.io/model-implementation-principles/) which
provides guidance for model-fitting APIs.


Specific standards for neural network algorithms have also been developed as
part of a [google 2019 Summer Of Code
project](http://www.inmodelia.com/gsoc2019.html), resulting in a dedicated
R package, [`NNbenchmark`](https://akshajverma.com/NNbenchmarkWeb/index.html),
and accompanying results---their so-called
["notebooks"](https://akshajverma.com/NNbenchmarkWeb/notebooks.html)---of
applying their benchmarks to a suite of neural network packages.

## Generally Applicable Standards 

The project aims to establish and maintain a set of standards governing general
aspects of software, such as software interfaces, documentation, and testing.

<!---
https://github.com/tdwg/vocab/blob/master/sds/documentation-specification.md
--->

<!---
Each debian release must include a space-separated list of bug report
   numbers closed by that release.
--->

### Documentation

Standards will include requirements for form and completeness of documentation.
As with interface, several sources already provide starting points for
reasonable documentation. Some documentation requirements will be specific to
the statistical context. For instance, it is likely we will have requirements
for referencing appropriate literature or references for theoretical support of
implementations. Another area of importance is correctness and clarity of
definitions of statistical quantities produced by the software, e.g., the
definition of null hypotheses or confidence intervals. Data included in
software -- that used in examples or tests -- will also have documentation
requirements. It is worth noting that the
[`roxygen`](https://roxygen2.r-lib.org/) system for documenting R packages is
readily extensible, as exemplified through the [`roxytest`
package](https://github.com/mikldk/roxytest) for specifying tests *in-line*.

### Input Structures

This section considers general standards for *Input Structures*. These
standards may often effectively be addressed through implementing class
structures, although this is not a general requirement. Developers are
nevertheless encouraged to examine the guide to [S3
vectors](https://vctrs.r-lib.org/articles/s3-vector.html#casting-and-coercion)
in the [`vctrs` package](https://vctrs.r-lib.org) as an example of the kind of
assurances and validation checks that are possible with regard to input data.
Systems like those demonstrated in that vignette provide a very effective way
to ensure that software remains robust to diverse and unexpected classes and
types of input data.

#### Uni-variate (Vector) Input

It is important to note for univariate data that single values in R are vectors
with a length of one, and that `1` is of exactly the same *data type* as `1:n`.
Given this, inputs expected to be univariate should:

- G2.0 Provide explicit secondary documentation of any expectations on lengths
  of inputs (generally implying identifying whether an input is expected to be
  single- or multi-valued)
- G2.1 Provide explicit secondary documentation of expectations on *data types*
  of all vector inputs (see the above list).
- G2.2 Appropriately prohibit or restrict submission of multivariate input to
  parameters expected to be univariate.
- G2.3 Provide appropriate mechanisms to convert between different *data
  types*, potentially including:
    - G2.3a explicit conversion to `integer` via `as.integer()`
    - G2.3b explicit conversion to continuous via `as.numeric()`
    - G2.3c explicit conversion to character via `as.character()` (and not
      `paste` or `paste0`)
    - G2.3d explicit conversion to factor via `as.factor()`
    - G2.3e explicit conversion from factor via `as...()` functions
- G2.4 Where inputs are expected to be of `factor` type, secondary
  documentation should explicitly state whether these should be `ordered` or
  not, and those inputs should provide appropriate error or other routines to
  ensure inputs follow these expectations.


#### Multivariate Input

This sub-section refers to "standard rectangular forms" for input data. The
fundamental rectangular form in R is an `array`, which is just a vector with
additional attributes specifying rectangular dimensions. A `matrix` was
technically a special form of `array` which allowed additional attributes such
as dimension (row and column) names, but since R4.0.0 the two have become
largely equivalent such that an `array` constructor creates an object of class
`c("matrix", "array")`, and `inherits(m, "array")` is true for a `matrix` `m`.
Both terms may accordingly be considered entirely equivalent.

Given this, multivariate inputs may be in one or or more of the following forms:

- `matrix`
- `data.frame`
- Extensions such as
    - [`tibble`](https://tibble.tidyverse.org)
    - [`data.table`](https://rdatatable.gitlab.io/data.table)
    - domain-specific classes such as
      [`tsibble`](https://tsibble.tidyverts.org) for time series

General Standards applicable to software which is intended to accept any one or
more of these rectangular inputs are then that:

- G2.5 Software should accept as input as many of the above standard forms as
  possible, including extension to domain-specific forms
- G2.6 Software should provide appropriate conversion routines as part of initial
  pre-processing to ensure that all other sub-functions of a package receive
  inputs of a single defined type.
- G2.7 Software should issue diagnostic messages for type conversion in which
  information is lost (such as conversion of variables from factor to
  character; or standardisation of variable names) or added (such as insertion
  of variable or column names where none were provided).

#### Missing or Undefined Values

- G2.8 Statistical Software should implement appropriate checks for missing
  data as part of initial pre-processing prior to passing data to analytic
  algorithms.
- G2.9 Where possible, all functions should provide options for users to
  specify how to handle missing (`NA`) data, with options minimally including:
  - G2.9a error on missing data
  - G2.9b ignore missing data with default warnings or messages issued
  - G2.9c replace missing data with appropriately imputed values
- G2.10 Functions should never assume non-missingness, and should never pass
  data with potential missing values to any base routines with default `na.rm =
  FALSE`-type parameters (such as
  [`mean()`](https://stat.ethz.ch/R-manual/R-devel/library/base/html/mean.html),
  [`sd()`](https://stat.ethz.ch/R-manual/R-devel/library/stats/html/sd.html) or
  [`cor()`](https://stat.ethz.ch/R-manual/R-devel/library/stats/html/cor.html)).
- G2.11 All functions should also appropriately handle undefined values 
  (e.g., `NaN`, `Inf` and `-Inf`), including potentially providing options for
  ignoring or removing such values.

### Output Structures

- G3.1 Statistical Software which enables outputs to be written to local files
  should parse parameters specifying file names to ensure appropriate file
  suffices are automatically generated where not provided.

### Testing {#standards-testing}

Testing is a critical area for standards, as tests are a concrete manifestation
of standards and the means by which authors may demonstrate compliance. While
testing is considered best practice and test coverage often used as a measure of
test completeness, guidance on *what* to test is rare, especially in the context
of R packages. Thus, standards will need to provide guidance on the types and
methods of tests required for different statistical software categories. The
following are likely key questions to note in relation to testing:

-   To what extent should testing focus on *functional* or *integration* rather
    than *unit* testing?

-   Is it sufficient to consider test execution as an integral part of
    `R CMD check` only? Or might there by a case for developing alternative test
    execution environments and approaches? For instance, should there be an
    alternate workflow for long-running tests, tests requiring large data, or
    tests intended to be executed for other purposes?

-   Is it worthwhile concretely defining one or more goals of testing? (Such as
    error detection, error frequencies, error tolerance, accuracy.)

-   What are the test data? And how easy is it to input alternative data to
    tests?

-   Is there scope for "stochastic" or "property-based" testing?

-   What test reporter should be used? Does the `testthat` package and similar
    suffice? Or might it be worth considering new test reporting systems?

-   What aspects of tests and test data (both actual and permissible) might be
    worthwhile documenting in some kind of metadata format?

Extant R package which address some of these issues include
[`tinytest`](https://github.com/markvanderloo/tinytest),
[`roxytest`](https://github.com/mikldk/roxytest), and
[`xpectr`](https://github.com/LudvigOlsen/xpectr).

All packages should follow rOpenSci standards on
[testing](https://devguide.ropensci.org/building.html#testing) and [continuous
integration](https://devguide.ropensci.org/ci.html), including aiming for high
test coverage.  For testing _statistical algorithms_, tests should include
tests of the following types:


-  **Correctness tests** to test that statistical algorithms produce expected
   output (predictions, fitted values) given test data sets.
    -  For new methods, it can be difficult to separate out correctness of the
      the method from the correctness of the implementation, as there may not
      be reference for comparison.  In this case, options include testing
      against simple, trivial cases or testing multiple implementations (e.g.,
      an initial R implementation then ported to C/C++.)
    - For new implementations of existing methods, correctness tests should
      include tests against previous implementations.  Such testing may
      explicitly call those implementations in testing, preferably from
      fixed-versions of other software, or use stored outputs from those where
      it is not possible.
        - Stored values may even be drawn from published paper outputs when
          applicable and code from the original implementation is not available
        - Note binding frameworks such as
          [RStata](https://github.com/lbraglia/RStata) may help here.)
    - Where applicable, using standard data sets with known properties, (e.g.,
      [NIST Standard Reference Datasets](https://www.itl.nist.gov/div898/strd/)
      is encouraged.
    - Correctness tests should be run with a fixed random seed

- **Parameter recovery tests** to test that the implementation produce expected
  results given data with known properties.  For instance, a linear regression
  algorithm should return expected coefficient values for a simulated data set
  generated from a linear model.  In general parameter recovery tests should be
  expected to succeed within a tolerance rather than exact values.
    - Parameter recovery tests should be run with multiple random seeds when
      either data simulation or the algorithm contains a random component


- **Algorithm performance tests** test that implementation performs as expected
  as properties of data change.  For instance, a test may show that parameters
  approach correct estimates within tolerance and data size increases.

-  **Edge condition tests** to test that these conditions produce expected
   behavior such as clear warnings or errors when confronted with data with
   extreme properties 
    -  Zero-length data, 
    -  Data of unsupported types (e.g., character or complex numbers in for
      functions designed only for numeric data)
    -  Data with all-NA fields or all identical fields
    -  Data outside the scope of the algorithm (e.g., data with more fields
      (columns) than observations (rows) for some regression)

- Packages should test for expected stochastic behavior, e.g., 
    - Adding trivial (e.g. `.Machine$double.eps`-scale) noise to data does not
      meaningfully change results
    - Running under differennt random seeds / initital conditions does not
      meaningfully change results

- Data sets used in testing should be made available to the user in the package. 


- Export data sets used for testing in the package for user confirmation and examples


#### Extended tests

Tests on large data, tests with many permutations and other conditions are
often ill-suited to workflows of unit tests run continuously with every code
change.  In these cases authors may wish to provided extended tests.

Extended tests should run under a common framework of with other tests but be
switched on by flags such as as a `MYPKG_EXTENDED_TESTS=1` environment
variable.  Where extended tests require large data sets or other assets, these
should be provided for downloaded and fetched as part of the testing workflow.

Conditions for running extended tests such as platform requirements, memory,
expected runtime, and artifacts produced that may need manual inspection,
should be described in developer documentation such as a `CONTRIBUTING.md`
file. 

### Demonstration of innovation, novelty, or advancement

The standards listed above largely refer to _minimum_ requirements for
software, but following common practice in academic publishing, one may also
possibly require that a piece of software is demonstrably superior to otherwise
equivalent software in at least one (or perhaps more?) specific way(s).  (In
current rOpenSci standards, packages are required to [demonstrate signicant
improvement](https://devguide.ropensci.org/policies.html#overlap) over similar
packages). If such a "relative improvement" requirement is included in the
process, authors may be required to demonstrate how their software exceeds
existing or reference implementations of similar tools in some of the following
ways:

-  **Efficiency:** Is the software more efficient (faster, simpler, other
   interpretations of "efficient") than reference implementations?
-  **Reproducibility or Reliability:** Does the software reproduce sufficiently
  similar results more frequently than reference implementations (or otherwise
  satisfy similar interpretations of reproducibility)?
-  **Accuracy or Precision:** Is the software demonstrably more accurate or
  precise than reference implementations (such as ?
-  **Simplicity of Use:** Is the software simpler to use than reference
  implementations?
-  **Algorithmic Characteristics:** Does the algorithmic implementation offer
  characteristics (such as greater simplicity or sensitivity) superior to
  reference implementations? If so, which?
-  **Convergence:** Does the software provide faster or otherwise better
  convergence properties than reference implementations?
-  **Method Validity:** Does the software overcome demonstrable flaws in
  previous (reference) implementations? If so, how?
-  **Method Applicability:** Does the software enable a statistical method to
  be applied to a domain in which such application was not previously possible?
-  **Automation** Does the software automate aspects of statistical analyses
  which previously (in a reference implementation) required manual
  intervention?
-  **Input Data:** Does the software "open up" a method to input data
  previously unable to be treated by a particular algorithm or method?
-  **Output Data:** Does the software provide output in forms previously
  unavailable by reference implementations?
-  **Reference Standards:** Are there any reference standards, such as the US
  National Institute of Standards and Technology's [collection of reference
  data sets](https://www.itl.nist.gov/div898/strd) against which the software
  may be compared? If so, which?

## Standards Specific to Statistical Software



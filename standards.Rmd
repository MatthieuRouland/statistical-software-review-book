
# General Standards {#standards}

The applicability of any concrete set of standards is likely to differ between
different categories of statistical. For example, metrics of numerical accuracy
will likely differ between categories primarily describing analytical algorithms
and those describing less tractable routines which produce less directly
reproducible results. Or consider metrics derived from tests, which must be
interpreted in *qualitatively* different ways for packages entirely dependent on
their own internal code versus packages largely dependent on the results of
calls to external data providers (along with additional differences between, for
example, locally-installed "external" providers versus online sources of
external data).

Different standards must thus be considered to be differentially applicable to
different categories of software, and thus the interplay between the scope of
statistical software considered above and throughout this project, and the
standards emerging from the project, will be of critical importance throughout
the project. Such considerations lead to the following kinds of questions which
will likely have to be addressed:

-   To what extent ought we aim for general standards at the expense of specific
    ability to assess particular categories of statistical software?

-   To what extent ought we strive for automation of software assessment, given
    the inherent risk of overseeing qualitative differences between different
    categories?

The following exemplify a few categories of statistical standards which may be
considered, emphasising restrictions of applicability to alernative kinds of
software.

-   **Numerical standards such as precision or convergence.** These will be
    applicable only to some restricted subset of all potential categories of
    statistical software (likely including but not limited to analytic and, to
    some extent, predictive routines) Moreover, even these two categories alone
    will likely require differing standards for precision or convergence.

-   **Method validity** It may be necessary or useful to develop standards for
    the *validity* of a chosen method, independent of its implementation.
    Questions of validity are commonly related to domains of application, and
    therefore must relate directly to any system for categorising statistical
    software. A method may (have been demonstrated to) be valid for some
    particular domain of application, and a software routine may be developed to
    adapt that method to some previously untried domain. It may then be
    necessary to consider potential (in)validity of that software, along with
    potential validity in other domains, themselves potentially not explicitly
    considered by the software authors.

-   **Software scope** The preceding considerations extend directly to general
    concerns of *scope*, whether in terms of domains of applicability,
    properties of input or output data, authorial intentions, or other
    contextual factors. Scope in all of these senses obviously must directly
    affect and determine the kinds of standards which may or may not apply to
    software, just as defining scope in these senses is also effectively an
    exercise in categorization of the kind described above.

-   **Reference standards** For software which implements or relies on standard
    routines, it may be necessary to designate reference data or
    implementations against which to compare outcomes, or guidance in selecting
    such references. For instance, the National Institute of Standards and
    Technology of the U.S. provides [a collection of reference data
    sets](https://www.itl.nist.gov/div898/strd/) with certified computational
    results which statistical software should be able to reproduce.

## Generally Applicable Standards 

In addition to standards specific to the challenges of statistical software, it
will also be necessary to create or adopt standards governing general aspects of
software, such as interface, documentation, and testing. @mili_software_2015
also provides a general list of software design principles, divided between
*Functional Attributes* (including Correctness and Robustness), *Usability
Attributes* (including Ease of Use, Ease of Learning, Customizability,
Calibrability, and Interoperability), and *Structural Attributes* (including
Design Integrity, Modularity, Testability, and Adaptability).

## Software Interface

There are likely aspects of overall software interface ("API") that might be
considered, reviewed, encouraged, or expected. rOpenSci's guide on [package
development, maintenance, and peer review](https://devguide.ropensci.org/)
provides standards of this type for R packages, primarily within its first
chapter. Another notable example is the [tidyverse design
guide](https://principles.tidyverse.org/), and [Conventions for R Modeling
Pacakges](https://tidymodels.github.io/model-implementation-principles/)
provides guidance for model-fitting APIs.

## Documentation

Standards will include requirements for form and completeness of documentation.
As with interface, several sources already provide starting points for
reasonable documentation.

Some documentation requirements will be specific to the statistical context. For
instance, it is likely we will have requirements for referencing appropriate
literature or references for theoretical support of implementations. Another
area of importance is correctness and clarity of definitions of statistical
quantities produced by the software, e.g., the definition of null hypotheses or
confidence intervals. Data included in software -- that used in examples or
tests -- will also have documentation requirements.

An additional area for consideration is the creation of tools for documentation
creation and evaluation based on metadata of statistical method inputs and
outputs and packaged data [@lenhardt_data_2014]. Relationships between data
and statistical software may be structured in a sufficiently systematic way to
permit systematic documentation.

<!---
https://github.com/tdwg/vocab/blob/master/sds/documentation-specification.md
--->

## Testing

Testing is a critical area for standards, as tests are a concrete manifestation
of standards and the means by which authors may demonstrate compliance. While
testing is considered best practice and test coverage often used as a measure of
test completeness, guidance on *what* to test is rare, especially in the context
of R packages. Thus, standards will need to provide guidance on the types and
methods of tests required for different statistical software categories.

In addition, statistical software may benefit from means or modes of testing
beyond the common frameworks used in and for R packages (e.g. R RMD check,
testhtat). A variety of other frameworks and workflows from other languages and
contexts may be relevant. Almost all testing as currently implemented in R is
"concrete testing" (Mili 2015), and little consideration has been given in R to
"stochastic" or "property-based" testing, in which expectation values of inputs
and outputs are tested, rather than concrete instantiations of such. Other
languages have developed grammars for stochastic or property-based testing,
notably through the [hypothesis package for
python](https://github.com/HypothesisWorks/hypothesis). These grammars enable
specification of test assumptions as well as expected test outputs. Assumptions
in `hypothesis` are declared through simple `@given` statements that might, for
example, quantify an assumed probability distribution for input data, while
outputs are specified through equivalent `@expect` statements that might, for
example, specify expected *distributional properties* of an output rather than
just concrete values.

The following are likely key questions which we will need to address regarding
testing:

-   To what extent should testing focus on *functional* or *integration* rather
    than *unit* testing?

-   Is it sufficient to consider test execution as an integral part of
    `R CMD check` only? Or might there by a case for developing alternative test
    execution environments and approaches? For instance, should there be an
    alternate workflow for long-running tests, tests requiring large data, or
    tests intended to be executed for other purposes?

-   Is it worthwhile concretely defining one or more goals of testing? (Such as
    error detection, error frequencies, error tolerance, accuracy.)

-   What are the test data? And how easy is it to input alternative data to
    tests?

-   Is there scope for "stochastic" or "property-based" testing?

-   What test reporter should be used? Does the `testthat` package and similar
    suffice? Or might it be worth considering new test reporting systems?

-   What aspects of tests and test data (both actual and permissible) might be
    worthwhile documenting in some kind of metadata format?

## General Software Metrics

The following is an incomplete list of the kinds of metrics commonly used to
evaluate software in general, and which might provide useful for assessing
statistical software in the present project.

-   Code structure

    -   Cyclomatic complexity

    -   Codebase size

    -   Function size / number

    -   Numbers of external calls within functions

    -   Numbers and proportions of Exported / non exported functions

    -   Code consistency

    -   Dynamic metrics derived from function call networks or similar

        -   Network-based metrics both for entire packages, for individual
            functions, and derived from analyses of test coverage

        -   Functional overlap with other packages

-   Documentation metrics:

    -   Numbers of documentation lines per function

    -   Proportion of documentation to code lines

    -   Presence of examples

    -   Vignettes

-   Data documentation metrics

    -   Intended and/or permitted kinds of input data

    -   Nature of output data

    -   Description of data used in tests

-   Meta struture

    -   Dependencies

    -   Reverse dependencies

-   Meta metrics

    -   License (type, availability, compatibility)

    -   Version control?

    -   Availability of website

    -   Availability of source code (beyond CRAN or similar)

    -   Community:

        -   Software downloads and usage statistics

        -   Numbers of active contributors

        -   Numbers or rates of issues reported

    -   Maintenance:

        -   Rate/Numbers of releases

        -   Rate of response to reported issues

        -   Last commit

        -   Commit rate

    -   stars (for github, or equivalent for other platforms)

    -   forks

-   Extent of testing

    -   Code coverage

    -   Examples and their coverage

    -   Range of inputs tested

## Metrics specific to statistical software

Metrics specific to statistical software will depend on, and vary in
applicability or relevance with, the system for categorizing statistical
software expected to emerge from the initial phase of this project. Details
of this sub-section will be largely deferred until we have a clearer view of
what categories might best be considered, which we are hopeful will emerge
following the first committee meeting, and in response to ensuing feedback. In
the meantime, metrics can be anticipated by referring to the preceding examples
for categories of statistical software (numerical standards, method validity,
software scope, and reference standards). We anticipate having a number of such
categories, along with a number of corresponding metrics for assessing software
in regard to each category. As mentioned at the outset, software will generally
be expected to fit within multiple categories, and specific metrics will need
to be developed to ensure validity for software encompassing any potential
combination of categories.

## Diagnostics and Reporting

While the preceding sub-sections considered *what* might be assessed in
relation to statistical software, the project will also need to explicitly
consider *how* any resultant assessment might best be presented and reported
upon. Indeed, a key output of the project is expected to be a suite of tools
which can be used both in this and other projects to construct, curate, and
report upon a suite of peer-reviewed software. Moreover, we will aim to develop
these tools partly to provide or enhance the *automation* of associated
processes, aiming both to enhance adaptability and transferability, and to
ensure the scalability of our own project.

It is useful in this context to distinguish between *collective* tools useful
for, of applicable to, collections of software, of individuals, or of processes
pertaining to either (here, primarily peer review), and *singular* tools of
direct applicability to individual pieces of software. We envision needing to
address the (likely relative) importance of some of the following kinds of
diagnostic and reporting tools which may be usefully developed.

**Collective Tools**

-   Qualitative tools useful in assessing or formalizing categories of software

-   Quantitative tools to retrospectively assess such aspects as:

    -   Collective "quality" of software

    -   Community engagement

    -   Effectiveness (or other metrics) of review

**Singular Tools**

-   Quantitative tools that can be prospectively used to

    -   Improve or assure software quality

    -   Document aspects of software quality

    -   Aid modularity or transferability either of software, or of the tools
        themselves

-   Tools to formalize structural aspects of software such as tests (for
    example, through implementing new frameworks or grammars)

-   Extensions of extant packages such as **lintr**, **covr**, **goodpractice**

-   Comparisons of package metrics to distributions for other packages or
    systems (such as the CRAN archive directories)

-   Diagnostic and report aggregation, design, or automatic creation at any
    stage before, during, or after peer review.

The one question of abiding importance is the extent to which any such tools,
and/or the automation of processes which they may enable, might enhance any of
the following aspects:

- Software development
- Peer review of software
- Wider communities of users or developers
- The adaptation of our system to other domains

A good example for the
effectiveness of automation in the kinds of peer review processes envisioned to
emerge from this project is provided by submissions to the [Journal of Open
Source Software](https://joss.theoj.org/), which features [open
reviews](https://github.com/openjournals/joss-reviews/issues), many aspects of
which are automated by a custom-developed bot called
["whedon"](https://github.com/whedon).


